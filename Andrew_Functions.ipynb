{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, metrics \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from keras.models import Model, Sequential\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.layers import Input, Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: none of these fxns are meant for single point input, input should always be a pd.DataFrame with more\n",
    "#than one row, not an unreasonable request since sklearn makes you reshape 1D arrays.\n",
    "\n",
    "\n",
    "# Data cleaning section: \n",
    "\n",
    "def split_and_scale(df_train, n):\n",
    "    \"\"\"Splits training dataframe into predictors and properties to be predicted and returns them in 2 new dfs.\n",
    "       This function assumes all of the predictors are grouped together on the right side of the df.\n",
    "       df_train: training df\n",
    "       n: number of properties to be predicted(number of outputs)\"\"\"\n",
    "    properties, predictors = split(df_train, n)\n",
    "    # COMMENT OUT THIS LINE IF YOU DONT WANT TO HAVE POLYNOMIAL TERMS IN YOUR TRAINING DATA\n",
    "    # But note that accuracy is much better with this, but the model will have higher variance\n",
    "    predictors_polynomial = polynomialize(predictors)\n",
    "    predictors_scaled_polynomial, predictors_scaler_polynomial = scaling(predictors_polynomial)\n",
    "    return properties, predictors_scaled_polynomial, predictors_scaler_polynomial \n",
    "\n",
    "\n",
    "def polynomialize(series):\n",
    "    \"\"\"Adds polynomial features to degree 3, including interaction features. \n",
    "    series: an input ndarray of floats to be polynomialized.\n",
    "    This function returns a ndarray of all of the features specified above.\"\"\"\n",
    "    # Creating polynomial object\n",
    "    poly = PolynomialFeatures(degree = 3)\n",
    "    # Adding polynomial terms\n",
    "    predictors_polynomial = poly.fit_transform(series)\n",
    "    return predictors_polynomial\n",
    "\n",
    "# Still in development, in case we want to add more terms that aren't polynomial\n",
    "# def add_nonlinear_terms(df, n):\n",
    "#     properties = df[df.columns[-n:]]\n",
    "#     predictors = df[df.columns[:-n]]\n",
    "#     i = np.arange(len(predictors.columns) * 4)\n",
    "#     x = 0\n",
    "#     for column in predictors.values:\n",
    "#         predictors.assign(i[x]=column**2)\n",
    "#         predictors.assign(column**3)\n",
    "#         predictors.assign(np.exp(column))\n",
    "#         predictors.assign(np.sign(column))\n",
    "#     return properties, predictors\n",
    "\n",
    "\n",
    "def split(df, n):\n",
    "    \"\"\"Takes an input pd.DataFrame and returns 2 ndarrays of the properties \n",
    "    and predictors.\"\"\"\n",
    "    properties = df[df.columns[-n:]].values\n",
    "    predictors = df[df.columns[:-n]].values\n",
    "    return properties, predictors\n",
    "\n",
    "\n",
    "def scaling(df_train):\n",
    "    \"\"\"This function takes a pd.DataFrame, creates a sklearn.StandardScaler, scales the DataFrame,\n",
    "       and returns the scaled data in a pd.DataFrame as well as the sklearn.StandardScaler object\n",
    "       for transforming data back to unscaled form post machine learning.\n",
    "       df_train: pd.DataFrame(for our purposes should be of shape 20 columns by an arbitrary number of rows)\"\"\"\n",
    "    #Creating scaler object\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    #Scaling df_train\n",
    "    scaled_data = pd.DataFrame(scaler.fit_transform(df_train))\n",
    "    \n",
    "    return scaled_data, scaler\n",
    "\n",
    "# Training/predicting\n",
    "\n",
    "\n",
    "def train_model(df_train, df_validation, model, n):\n",
    "    \"\"\"This function takes a training DataFrame, validation DataFrame and a preconfigured model\n",
    "       and trains said model on the training data followed by measuring error on validation data and \n",
    "       returning both the trained model and accuracy metric. This function assumes whatever parameter(s)\n",
    "       being predicted is in the last column(s) of df_train.\n",
    "       n: number of outputs\n",
    "       because this function returns the trained model, more metrics can be performed later that are specific\n",
    "       to whatever package it is in/the type of model it is\n",
    "       Parameters\"\"\"\n",
    "    #generating scaled data and their respective scaler objects\n",
    "    t_properties, t_predictors_scaled, t_predictors_scaler = split_and_scale(df_train, n)\n",
    "    v_properties, v_predictors_scaled, v_predictors_scaler = split_and_scale(df_validation, n)\n",
    "    #supervised learning of predictors and properties to fit model, note: keras does not take pd.DataFrames for\n",
    "    #training, using .values fixes this\n",
    "    model.fit(t_predictors_scaled, t_properties)\n",
    "    #predicting output of validation set\n",
    "    predictions = pd.DataFrame(model.predict(v_predictors_scaled))\n",
    "    #calculating RMSE from sklearn package\n",
    "    val_error = np.sqrt(metrics.mean_squared_error(predictions, v_properties))\n",
    "    return model, val_error, t_predictors_scaler\n",
    "\n",
    "\n",
    "def model_prediction(test_data, fitted_model, scaler, n):\n",
    "    \"\"\"Takes a fitted model and predicts the output of test data, returns the predicted data and accuracy.\n",
    "       THIS FUNCTION IS ONLY TO BE USED FOR FUTURE PREDICTIONS OR TESTING(WHICH SHOULD ONLY BE DONE ONCE).\n",
    "       Do not use this while training a model, that's what the validation data will be used for. We do not \n",
    "       want to introduce bias into our model by fitting to the test data\n",
    "       n = number of predictors\"\"\"\n",
    "    \n",
    "    #splitting predictors and properties\n",
    "    properties, predictors = split(test_data, n)\n",
    "    predictors = polynomialize(predictors)\n",
    "    predictors_scaled = scaler.transform(predictors)\n",
    "    #predicting based on scaled input predictors\n",
    "    prediction = fitted_model.predict(predictors_scaled)\n",
    "    #calculating MSE\n",
    "    accuracy_metric = np.sqrt(metrics.mean_squared_error(properties, prediction))\n",
    "\n",
    "    return prediction, accuracy_metric\n",
    "\n",
    "# Below functions initialize all the different types of models we are looking at:\n",
    "\n",
    "\n",
    "def neural_network():\n",
    "    \"\"\"Creates a neural network object to be passed into train_model function, can change properties of net\n",
    "       here.\"\"\"\n",
    "    def model():\n",
    "        model = Sequential()\n",
    "        model.add(Dense(84, input_dim=84, kernel_initializer='normal', activation='relu'))\n",
    "        model.add(Dense(84, kernel_initializer='normal', activation='relu'))\n",
    "        model.add(Dense(1, kernel_initializer = 'normal'))#kernel_initializer = initial values of outputs i think\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        return model\n",
    "    network = KerasRegressor(build_fn=model, epochs=100, batch_size=25000, verbose=0)\n",
    "#     network.fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, \n",
    "    return network\n",
    "\n",
    "\n",
    "def linear_regression():\n",
    "    \"\"\"creates a linear regression object\"\"\"\n",
    "    regr = LinearRegression()\n",
    "    return regr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to go in cleaning section\n",
    "# import clean\n",
    "\n",
    "def test_split():\n",
    "    data = {'column1': [2, 2, 3], 'column2': [1, 3, 5]}\n",
    "    df = pd.DataFrame(data)\n",
    "    one, two = clean.split(df, 1)\n",
    "    assert one[0] == 1\n",
    "    assert two[0] == 2\n",
    "    return\n",
    "\n",
    "def test_scaling():\n",
    "    data = {'column1': [2.0, 2.0, 3.0], 'column2': [1.0, 3.0, 5.0]}\n",
    "    df = pd.DataFrame(data)\n",
    "    df, scaler = clean.scaling(df)\n",
    "    assert df.loc[0].iloc[0] == 0\n",
    "    assert df.loc[2].iloc[0] == 1\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>SMILES_str</th>\n",
       "      <th>stoich_str</th>\n",
       "      <th>mass</th>\n",
       "      <th>pce</th>\n",
       "      <th>voc</th>\n",
       "      <th>jsc</th>\n",
       "      <th>e_homo_alpha</th>\n",
       "      <th>e_gap_alpha</th>\n",
       "      <th>e_lumo_alpha</th>\n",
       "      <th>tmp_smiles_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>655365</td>\n",
       "      <td>C1C=CC=C1c1cc2[se]c3c4occc4c4nsnc4c3c2cn1</td>\n",
       "      <td>C18H9N3OSSe</td>\n",
       "      <td>394.3151</td>\n",
       "      <td>5.161953</td>\n",
       "      <td>0.867601</td>\n",
       "      <td>91.567575</td>\n",
       "      <td>-5.467601</td>\n",
       "      <td>2.022944</td>\n",
       "      <td>-3.444656</td>\n",
       "      <td>C1=CC=C(C1)c1cc2[se]c3c4occc4c4nsnc4c3c2cn1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1245190</td>\n",
       "      <td>C1C=CC=C1c1cc2[se]c3c(ncc4ccccc34)c2c2=C[SiH2]...</td>\n",
       "      <td>C22H15NSeSi</td>\n",
       "      <td>400.4135</td>\n",
       "      <td>5.261398</td>\n",
       "      <td>0.504824</td>\n",
       "      <td>160.401549</td>\n",
       "      <td>-5.104824</td>\n",
       "      <td>1.630750</td>\n",
       "      <td>-3.474074</td>\n",
       "      <td>C1=CC=C(C1)c1cc2[se]c3c(ncc4ccccc34)c2c2=C[SiH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21847</td>\n",
       "      <td>C1C=c2ccc3c4c[nH]cc4c4c5[SiH2]C(=Cc5oc4c3c2=C1...</td>\n",
       "      <td>C24H17NOSi</td>\n",
       "      <td>363.4903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>197.474780</td>\n",
       "      <td>-4.539526</td>\n",
       "      <td>1.462158</td>\n",
       "      <td>-3.077368</td>\n",
       "      <td>C1=CC=C(C1)C1=Cc2oc3c(c2[SiH2]1)c1c[nH]cc1c1cc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65553</td>\n",
       "      <td>[SiH2]1C=CC2=C1C=C([SiH2]2)C1=Cc2[se]ccc2[SiH2]1</td>\n",
       "      <td>C12H12SeSi3</td>\n",
       "      <td>319.4448</td>\n",
       "      <td>6.138294</td>\n",
       "      <td>0.630274</td>\n",
       "      <td>149.887545</td>\n",
       "      <td>-5.230274</td>\n",
       "      <td>1.682250</td>\n",
       "      <td>-3.548025</td>\n",
       "      <td>C1=CC2=C([SiH2]1)C=C([SiH2]2)C1=Cc2[se]ccc2[Si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>720918</td>\n",
       "      <td>C1C=c2c3ccsc3c3[se]c4cc(oc4c3c2=C1)C1=CC=CC1</td>\n",
       "      <td>C20H12OSSe</td>\n",
       "      <td>379.3398</td>\n",
       "      <td>1.991366</td>\n",
       "      <td>0.242119</td>\n",
       "      <td>126.581347</td>\n",
       "      <td>-4.842119</td>\n",
       "      <td>1.809439</td>\n",
       "      <td>-3.032680</td>\n",
       "      <td>C1=CC=C(C1)c1cc2[se]c3c4sccc4c4=CCC=c4c3c2o1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                         SMILES_str   stoich_str  \\\n",
       "0   655365          C1C=CC=C1c1cc2[se]c3c4occc4c4nsnc4c3c2cn1  C18H9N3OSSe   \n",
       "1  1245190  C1C=CC=C1c1cc2[se]c3c(ncc4ccccc34)c2c2=C[SiH2]...  C22H15NSeSi   \n",
       "2    21847  C1C=c2ccc3c4c[nH]cc4c4c5[SiH2]C(=Cc5oc4c3c2=C1...   C24H17NOSi   \n",
       "3    65553   [SiH2]1C=CC2=C1C=C([SiH2]2)C1=Cc2[se]ccc2[SiH2]1  C12H12SeSi3   \n",
       "4   720918       C1C=c2c3ccsc3c3[se]c4cc(oc4c3c2=C1)C1=CC=CC1   C20H12OSSe   \n",
       "\n",
       "       mass       pce       voc         jsc  e_homo_alpha  e_gap_alpha  \\\n",
       "0  394.3151  5.161953  0.867601   91.567575     -5.467601     2.022944   \n",
       "1  400.4135  5.261398  0.504824  160.401549     -5.104824     1.630750   \n",
       "2  363.4903  0.000000  0.000000  197.474780     -4.539526     1.462158   \n",
       "3  319.4448  6.138294  0.630274  149.887545     -5.230274     1.682250   \n",
       "4  379.3398  1.991366  0.242119  126.581347     -4.842119     1.809439   \n",
       "\n",
       "   e_lumo_alpha                                     tmp_smiles_str  \n",
       "0     -3.444656        C1=CC=C(C1)c1cc2[se]c3c4occc4c4nsnc4c3c2cn1  \n",
       "1     -3.474074  C1=CC=C(C1)c1cc2[se]c3c(ncc4ccccc34)c2c2=C[SiH...  \n",
       "2     -3.077368  C1=CC=C(C1)C1=Cc2oc3c(c2[SiH2]1)c1c[nH]cc1c1cc...  \n",
       "3     -3.548025  C1=CC2=C([SiH2]1)C=C([SiH2]2)C1=Cc2[se]ccc2[Si...  \n",
       "4     -3.032680       C1=CC=C(C1)c1cc2[se]c3c4sccc4c4=CCC=c4c3c2o1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('HCEPD_100K.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data[['e_homo_alpha', 'jsc', 'e_gap_alpha', 'e_lumo_alpha', 'mass','voc', 'pce']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[5.16195320e+00],\n",
       "        [5.26139772e+00],\n",
       "        [5.32907052e-15],\n",
       "        [6.13829370e+00],\n",
       "        [1.99136566e+00]]), 5.02821956683809e-15)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linear regression test - accuracy is 100x better after adding polynomial terms\n",
    "model, val_error, scaler = train_model(sample.iloc[:80000], sample.iloc[80000:],linear_regression(), 1)\n",
    "model_prediction(sample.iloc[0:5], model, scaler, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network test, make sure to update your layer for expected input\n",
    "model1, val_error1, scaler1 = train_model(sample.iloc[:80000], sample.iloc[80000:],neural_network(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5.238229 , 5.182736 , 0.5232755, 6.0541496, 2.0207558],\n",
       "       dtype=float32), 0.24239096160995363)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_prediction(sample.iloc[0:5], model1, scaler1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO REDUCE OVERFITTING: reduce degree of polynomial terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model might just not have the right things\n",
    "#to reorder columns:\n",
    "#cols = df.columns.tolist()\n",
    "#df = df[cols] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matminer.data_retrieval.retrieve_MP import MPDataRetrieval\n",
    "mpdr = MPDataRetrieval(api_key='x5He3oeSg1eCaIU4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xrd_data(xrd_data):\n",
    "    \"\"\"\n",
    "    Extracts the relevant XRD data from the dictionary obtained from MPD\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    xrd_data : Dictionary\n",
    "      The dictionary of data obtained from MPD \n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    clean_df: Pandas dataframe\n",
    "        The top 10 XRD peaks and their corresponding two theta values for the material\n",
    "    \"\"\"\n",
    "    \n",
    "    # Checking if data has 10 or more peaks\n",
    "    # Will return nothing and skip the material if less than 10\n",
    "    if len(xrd_data['Cu']['pattern']) >= 10:\n",
    "        \n",
    "        # Extracting out the amplitude and two theta values from the dictionary contained inside the received data\n",
    "        # then turning it into a pandas dataframe.\n",
    "        dirty_df = pd.DataFrame(xrd_data['Cu']['pattern'], columns=xrd_data['Cu']['meta']) # Converts data into dataframe\n",
    "        dirty_df.drop(['hkl','d_spacing'], axis=1, inplace=True) # Disposes of the hkl and d-spacing data\n",
    "\n",
    "        # Sorting the peaks into the top 10 with the highest peaks\n",
    "        dirty_df.sort_values('amplitude', ascending=False, inplace=True) # Sorts peaks from highest to smallest\n",
    "        dirty_df.reset_index(drop=True, inplace=True) # Reseting index\n",
    "        clean_df = dirty_df[:10] # Dropping all peaks below the top ten \n",
    "        \n",
    "        return clean_df\n",
    "    \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reformat the data after cleaning\n",
    "# Takes the dataframe and turns it into a dictionary wwhere all data points have a unique key\n",
    "def reformat_xrd_data(xrd_data):\n",
    "    \"\"\"\n",
    "    Reformats the cleaned data obtained from the extract_xrd_data function into a dictionary\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    xrd_data : Dictionary\n",
    "      The dictionary of data obtained from MPD \n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    clean_df: Pandas dataframe\n",
    "        The top 10 XRD peaks and their corresponding two theta values for the material\n",
    "    \"\"\"\n",
    "    \n",
    "    # Checks if data was returned from the extracted data function\n",
    "    # Skips material if nothing is returned\n",
    "    if isinstance(extract_xrd_data(xrd_data), pd.DataFrame):\n",
    "        # Cleaning data and creating empty dictionary\n",
    "        clean_df = extract_xrd_data(xrd_data)\n",
    "        mat_dict = {}\n",
    "\n",
    "        # Loop to assign each data point to a key and stores it within the dictionary\n",
    "        for i in range(0,20):\n",
    "            if i < 10:\n",
    "                amp_key = ('amplitude_' + str(i))\n",
    "                mat_dict[amp_key] = clean_df['amplitude'][i]\n",
    "\n",
    "            else:\n",
    "                theta_key = ('two_theta_' + str(i-10))\n",
    "                mat_dict[theta_key] = clean_df['two_theta'][i-10]\n",
    "\n",
    "        return mat_dict\n",
    "    \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function \n",
    "def produce_data(df):\n",
    "    \"\"\"\n",
    "    Reformats the cleaned data obtained from the extract_xrd_data function into a dictionary\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    xrd_data : Dictionary\n",
    "      The dictionary of data obtained from MPD \n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    clean_df: Pandas dataframe\n",
    "        The top 10 XRD peaks and their corresponding two theta values for the material\n",
    "    \"\"\"\n",
    "    \n",
    "    full_dict = {}\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        \n",
    "        if reformat_xrd_data(df['xrd'].iloc[i]) != None:\n",
    "            ID = df.index[i]\n",
    "            mat_dict = reformat_xrd_data(df['xrd'].iloc[i])\n",
    "            full_dict[ID] = mat_dict\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    full_df = pd.DataFrame.from_dict(full_dict, orient='index')\n",
    "    \n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amplitude_0</th>\n",
       "      <th>amplitude_1</th>\n",
       "      <th>amplitude_2</th>\n",
       "      <th>amplitude_3</th>\n",
       "      <th>amplitude_4</th>\n",
       "      <th>amplitude_5</th>\n",
       "      <th>amplitude_6</th>\n",
       "      <th>amplitude_7</th>\n",
       "      <th>amplitude_8</th>\n",
       "      <th>amplitude_9</th>\n",
       "      <th>two_theta_0</th>\n",
       "      <th>two_theta_1</th>\n",
       "      <th>two_theta_2</th>\n",
       "      <th>two_theta_3</th>\n",
       "      <th>two_theta_4</th>\n",
       "      <th>two_theta_5</th>\n",
       "      <th>two_theta_6</th>\n",
       "      <th>two_theta_7</th>\n",
       "      <th>two_theta_8</th>\n",
       "      <th>two_theta_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mp-1001113</th>\n",
       "      <td>100.0</td>\n",
       "      <td>78.288443</td>\n",
       "      <td>45.261343</td>\n",
       "      <td>40.440821</td>\n",
       "      <td>30.707488</td>\n",
       "      <td>26.515207</td>\n",
       "      <td>22.092113</td>\n",
       "      <td>21.737670</td>\n",
       "      <td>16.821364</td>\n",
       "      <td>16.690467</td>\n",
       "      <td>36.834948</td>\n",
       "      <td>46.091710</td>\n",
       "      <td>38.160738</td>\n",
       "      <td>171.874808</td>\n",
       "      <td>45.443514</td>\n",
       "      <td>176.442253</td>\n",
       "      <td>60.797841</td>\n",
       "      <td>148.648332</td>\n",
       "      <td>87.640251</td>\n",
       "      <td>80.839115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mp-1056579</th>\n",
       "      <td>100.0</td>\n",
       "      <td>91.828984</td>\n",
       "      <td>79.970463</td>\n",
       "      <td>45.423897</td>\n",
       "      <td>30.872127</td>\n",
       "      <td>26.312484</td>\n",
       "      <td>23.760237</td>\n",
       "      <td>15.234896</td>\n",
       "      <td>14.045885</td>\n",
       "      <td>13.052846</td>\n",
       "      <td>44.350136</td>\n",
       "      <td>167.464337</td>\n",
       "      <td>37.945623</td>\n",
       "      <td>73.006051</td>\n",
       "      <td>140.704562</td>\n",
       "      <td>54.746898</td>\n",
       "      <td>123.597176</td>\n",
       "      <td>85.886608</td>\n",
       "      <td>135.942882</td>\n",
       "      <td>93.269152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mp-10649</th>\n",
       "      <td>100.0</td>\n",
       "      <td>35.031568</td>\n",
       "      <td>24.901660</td>\n",
       "      <td>23.115411</td>\n",
       "      <td>22.516940</td>\n",
       "      <td>20.830194</td>\n",
       "      <td>16.146856</td>\n",
       "      <td>15.585780</td>\n",
       "      <td>15.106204</td>\n",
       "      <td>14.134877</td>\n",
       "      <td>43.934318</td>\n",
       "      <td>37.766028</td>\n",
       "      <td>130.152767</td>\n",
       "      <td>39.420530</td>\n",
       "      <td>72.471977</td>\n",
       "      <td>83.797856</td>\n",
       "      <td>55.734788</td>\n",
       "      <td>71.487094</td>\n",
       "      <td>121.359663</td>\n",
       "      <td>87.842079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mp-1072544</th>\n",
       "      <td>100.0</td>\n",
       "      <td>74.250236</td>\n",
       "      <td>32.864700</td>\n",
       "      <td>32.201109</td>\n",
       "      <td>18.813035</td>\n",
       "      <td>17.053640</td>\n",
       "      <td>15.411324</td>\n",
       "      <td>14.668014</td>\n",
       "      <td>14.527636</td>\n",
       "      <td>13.937115</td>\n",
       "      <td>173.362106</td>\n",
       "      <td>18.640121</td>\n",
       "      <td>46.743236</td>\n",
       "      <td>32.580631</td>\n",
       "      <td>160.193335</td>\n",
       "      <td>50.741974</td>\n",
       "      <td>54.524230</td>\n",
       "      <td>42.462094</td>\n",
       "      <td>26.480052</td>\n",
       "      <td>98.859603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mp-1079297</th>\n",
       "      <td>100.0</td>\n",
       "      <td>28.393003</td>\n",
       "      <td>27.930625</td>\n",
       "      <td>27.680183</td>\n",
       "      <td>27.341183</td>\n",
       "      <td>24.356947</td>\n",
       "      <td>23.828151</td>\n",
       "      <td>21.640950</td>\n",
       "      <td>21.245367</td>\n",
       "      <td>20.805431</td>\n",
       "      <td>27.389812</td>\n",
       "      <td>25.843727</td>\n",
       "      <td>50.463312</td>\n",
       "      <td>46.935059</td>\n",
       "      <td>171.425514</td>\n",
       "      <td>51.286901</td>\n",
       "      <td>20.161639</td>\n",
       "      <td>52.606616</td>\n",
       "      <td>17.884802</td>\n",
       "      <td>177.432037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            amplitude_0  amplitude_1  amplitude_2  amplitude_3  amplitude_4  \\\n",
       "mp-1001113        100.0    78.288443    45.261343    40.440821    30.707488   \n",
       "mp-1056579        100.0    91.828984    79.970463    45.423897    30.872127   \n",
       "mp-10649          100.0    35.031568    24.901660    23.115411    22.516940   \n",
       "mp-1072544        100.0    74.250236    32.864700    32.201109    18.813035   \n",
       "mp-1079297        100.0    28.393003    27.930625    27.680183    27.341183   \n",
       "\n",
       "            amplitude_5  amplitude_6  amplitude_7  amplitude_8  amplitude_9  \\\n",
       "mp-1001113    26.515207    22.092113    21.737670    16.821364    16.690467   \n",
       "mp-1056579    26.312484    23.760237    15.234896    14.045885    13.052846   \n",
       "mp-10649      20.830194    16.146856    15.585780    15.106204    14.134877   \n",
       "mp-1072544    17.053640    15.411324    14.668014    14.527636    13.937115   \n",
       "mp-1079297    24.356947    23.828151    21.640950    21.245367    20.805431   \n",
       "\n",
       "            two_theta_0  two_theta_1  two_theta_2  two_theta_3  two_theta_4  \\\n",
       "mp-1001113    36.834948    46.091710    38.160738   171.874808    45.443514   \n",
       "mp-1056579    44.350136   167.464337    37.945623    73.006051   140.704562   \n",
       "mp-10649      43.934318    37.766028   130.152767    39.420530    72.471977   \n",
       "mp-1072544   173.362106    18.640121    46.743236    32.580631   160.193335   \n",
       "mp-1079297    27.389812    25.843727    50.463312    46.935059   171.425514   \n",
       "\n",
       "            two_theta_5  two_theta_6  two_theta_7  two_theta_8  two_theta_9  \n",
       "mp-1001113   176.442253    60.797841   148.648332    87.640251    80.839115  \n",
       "mp-1056579    54.746898   123.597176    85.886608   135.942882    93.269152  \n",
       "mp-10649      83.797856    55.734788    71.487094   121.359663    87.842079  \n",
       "mp-1072544    50.741974    54.524230    42.462094    26.480052    98.859603  \n",
       "mp-1079297    51.286901    20.161639    52.606616    17.884802   177.432037  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = mpdr.get_dataframe(criteria='Si', properties=['xrd'])\n",
    "produce_data(df).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
