{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from sklearn import preprocessing, metrics \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from keras.models import Model, Sequential\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.regularizers import Regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: none of these fxns are meant for single point input, input should always be a pd.DataFrame with more\n",
    "#than one row, not an unreasonable request since sklearn makes you reshape 1D arrays.\n",
    "\n",
    "def single_plot(x, y, figsize, title, xtitle, ytitle, xticks, yticks):\n",
    "    \"\"\"Plots a single figure.\n",
    "    x: x axis data\n",
    "    y: y axis data\n",
    "    figsize: tuple of format (xdim, ydim)\n",
    "    title: tuple of format (title(string), fontsize(int))\n",
    "    xtitle and ytitle similar to title\n",
    "    xticks and yticks: tuple of format (array of tick values, fontsize)\"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize = figsize)\n",
    "    plt.title(title[0], size = title[1])\n",
    "    plt.xticks(xticks[0], size = xticks[1])\n",
    "    plt.yticks(yticks[0], size = yticks[1])\n",
    "    plt.xlabel(xtitle[0], size = xtitle[1])\n",
    "    plt.ylabel(ytitle[0], size = ytitle[1])\n",
    "    ax1.set_xlim(0, 6)\n",
    "    ax1.scatter(x, y, marker = '.', alpha = .7)\n",
    "    return\n",
    "\n",
    "\n",
    "def split_and_scale(df, n, yes):\n",
    "    \"\"\"Splits training dataframe into predictors and properties to be predicted and returns them in 2 new dfs.\n",
    "       This function assumes all of the predictors are grouped together on the right side of the df.\n",
    "       df_train: training df\n",
    "       n: number of properties to be predicted(number of outputs)\"\"\"\n",
    "    # Splitting into properties and predictors\n",
    "    properties, predictors = split(df, n)\n",
    "    # Adding polynomial term columns\n",
    "    predictors_polynomial = polynomialize(predictors, yes)\n",
    "    # Scaling predictor data\n",
    "    predictors_scaled_polynomial, predictors_scaler_polynomial = scaling(predictors_polynomial)\n",
    "    return properties, predictors_scaled_polynomial, predictors_scaler_polynomial \n",
    "\n",
    "\n",
    "def polynomialize(series, yes):\n",
    "    \"\"\"Adds polynomial features to degree 3, including interaction features. \n",
    "    series: an input ndarray of floats to be polynomialized.\n",
    "    This function returns a ndarray of all of the features specified above.\n",
    "    \n",
    "    series: dataframe to be polynomialized\n",
    "    yes: list, array or tuple of the form:\n",
    "    (Bool deciding whether to add polynomial terms, degree of highest polynomial, bool deciding whether to only provide interaction terms)\n",
    "    Returns the polynomialized series.\"\"\"\n",
    "    # Creating polynomial object\n",
    "    if yes[0]:\n",
    "        poly = PolynomialFeatures(degree = yes[1], interaction_only = yes[2])\n",
    "        # Adding polynomial terms\n",
    "        series = poly.fit_transform(series)\n",
    "    return series\n",
    "\n",
    "\n",
    "def split(df, n):\n",
    "    \"\"\"Takes an input pd.DataFrame and returns 2 ndarrays of the properties \n",
    "    and predictors.\"\"\"\n",
    "    properties = df[df.columns[-n:]].values\n",
    "    predictors = df[df.columns[:-n]].values\n",
    "    return properties, predictors\n",
    "\n",
    "\n",
    "def scaling(df_train):\n",
    "    \"\"\"This function takes a pd.DataFrame, creates a sklearn.StandardScaler, scales the DataFrame,\n",
    "       and returns the scaled data in a pd.DataFrame as well as the sklearn.StandardScaler object\n",
    "       for transforming data back to unscaled form post machine learning.\n",
    "       df_train: pd.DataFrame(for our purposes should be of shape 20 columns by an arbitrary number of rows)\n",
    "       \n",
    "       Returns scaled dataframe and its respective scaler\"\"\"\n",
    "    #Creating scaler object\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    #Scaling df_train\n",
    "    scaled_data = pd.DataFrame(scaler.fit_transform(df_train))\n",
    "    return scaled_data, scaler\n",
    "\n",
    "\n",
    "def train_model(df_train, df_validation, model, n, yes):\n",
    "    \"\"\"This function takes a training DataFrame, validation DataFrame and a preconfigured model\n",
    "       and trains said model on the training data followed by measuring error on validation data and \n",
    "       returning both the trained model and accuracy metric. This function assumes whatever parameter(s)\n",
    "       being predicted is in the last column(s) of df_train.\n",
    "       n: number of outputs\n",
    "       df_validation: to measure accuracy\n",
    "       model: pre initialized model object\n",
    "       yes: list, array or tuple of the form:\n",
    "       (Bool deciding whether to add polynomial terms, degree of highest polynomial, bool deciding whether to only provide interaction terms)\n",
    "       because this function returns the trained model, more metrics can be performed later that are specific\n",
    "       to whatever package it is in/the type of model it is\n",
    "       Returns the model object, RMSE on validation set and the scaler for predictors\n",
    "       Note: can only predict data which has been scaled with the scaler this function returns\"\"\"\n",
    "    #generating scaled data and their respective scaler objects\n",
    "    t_properties, t_predictors_scaled, t_predictors_scaler = split_and_scale(df_train, n, yes)\n",
    "    v_properties, v_predictors_scaled, v_predictors_scaler = split_and_scale(df_validation, n, yes)\n",
    "    #supervised learning of predictors and properties to fit model, note: keras does not take pd.DataFrames for\n",
    "    #training, using .values fixes this\n",
    "    model.fit(t_predictors_scaled, t_properties)\n",
    "    #predicting output of validation set\n",
    "    predictions = pd.DataFrame(model.predict(v_predictors_scaled))\n",
    "    #calculating RMSE from sklearn package\n",
    "    val_error = np.sqrt(metrics.mean_squared_error(predictions, v_properties))\n",
    "    return model, val_error, t_predictors_scaler\n",
    "\n",
    "\n",
    "def model_prediction(test_data, fitted_model, scaler, n, yes):\n",
    "    \"\"\"Takes a fitted model and predicts the output of test data, returns the predicted data and accuracy.\n",
    "       THIS FUNCTION IS ONLY TO BE USED FOR FUTURE PREDICTIONS OR TESTING(WHICH SHOULD ONLY BE DONE ONCE).\n",
    "       Do not use this while training a model, that's what the validation data will be used for. We do not \n",
    "       want to introduce bias into our model by fitting to the test data\n",
    "       n = number of predictors\"\"\"\n",
    "    #splitting predictors and properties\n",
    "    properties, predictors = split(test_data, n)\n",
    "    predictors = polynomialize(predictors, yes)\n",
    "    predictors_scaled = scaler.transform(predictors)\n",
    "    #predicting based on scaled input predictors\n",
    "    prediction = fitted_model.predict(predictors_scaled)\n",
    "    #calculating MSE\n",
    "    accuracy_metric = np.sqrt(metrics.mean_squared_error(properties, prediction))\n",
    "\n",
    "    return prediction, accuracy_metric\n",
    "\n",
    "\n",
    "def neural_network(input_dimension):\n",
    "    \"\"\"Creates a neural network object to be passed into train_model function, can change properties of net\n",
    "       here. Alternatively, a neural network object can be created using Keras and passed to the train_model \n",
    "       function to be fitted.\n",
    "       \n",
    "       input_dimension: the dimensionality of the input predictors\"\"\"\n",
    "    def model():\n",
    "        model = Sequential()\n",
    "        model.add(Dense(1, input_dim=input_dimension, kernel_initializer='normal', activation='relu'))\n",
    "        model.add(Dense(20, kernel_initializer='normal', activation='relu'))\n",
    "        model.add(Dense(1, kernel_initializer = 'normal'))#kernel_initializer = initial values of outputs i think\n",
    "        opt = optimizers.Nadam(lr=0.5, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "        model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "        return model\n",
    "    # Creating neural network object\n",
    "    network = KerasRegressor(build_fn=model, epochs=150, batch_size=1, verbose=1)\n",
    "    return network\n",
    "\n",
    "\n",
    "def linear_regression():\n",
    "    \"\"\"Creates a linear regression object\n",
    "       to be passed to the train_model fxn.\"\"\"\n",
    "    regr = LinearRegression()\n",
    "    return regr\n",
    "\n",
    "def coefficient_statistics(df):\n",
    "    \"\"\"Creates a linear regression model using statsmodels, used to get\n",
    "       p values, confidence intervals and other metadata for models.\n",
    "       This function has too specific of a use case for a test function.\"\"\"\n",
    "    fit_object = smf.ols(formula='band_gap ~ amplitude_0 + amplitude_1 + amplitude_2 + amplitude_3 + amplitude_4 + amplitude_5 + amplitude_6 + amplitude_7 + amplitude_8 + amplitude_9+ two_theta_1+ two_theta_2 + two_theta_3 + two_theta_4 + two_theta_5 + two_theta_6 + two_theta_7 + two_theta_8 + two_theta_9', data=df)\n",
    "    ft = fit_object.fit()\n",
    "    return ft.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "def test_split():\n",
    "    data = {'column1': [2, 2, 3], 'column2': [1, 3, 5]}\n",
    "    df = pd.DataFrame(data)\n",
    "    one, two = xrdos.split(df, 1)\n",
    "    assert one[0] == 1\n",
    "    assert two[0] == 2\n",
    "    return\n",
    "\n",
    "def test_scaling():\n",
    "    data = {'column1': [2.0, 2.0, 3.0], 'column2': [1.0, 3.0, 5.0]}\n",
    "    df = pd.DataFrame(data)\n",
    "    df, scaler = scaling(df)\n",
    "    assert df.loc[0].iloc[0] == 0\n",
    "    assert df.loc[2].iloc[0] == 1\n",
    "    return\n",
    "\n",
    "def test_linear_regression():\n",
    "    regr = xrdos.linear_regression()\n",
    "    x = np.array([0.5, 1.0, 2.0])\n",
    "    y = np.array([0.5, 1.0, 2.0])\n",
    "    regr.fit(x.reshape(-1,1), y.reshape(-1,1))\n",
    "    p = np.array([0.5, 1.0, 2.0]).reshape(-1,1)\n",
    "    prediction = regr.predict(p)\n",
    "    for i in range(len(prediction)):\n",
    "        assert int(prediction[i]) == int(x[i])\n",
    "    assert type(p) == np.ndarray\n",
    "    return\n",
    "\n",
    "def test_neural_network():\n",
    "    assert type(xrdos.neural_network()) == keras.wrappers.scikit_learn.KerasRegressor\n",
    "    return\n",
    "\n",
    "def test_split_and_scale():\n",
    "    data = {'column1': [2, 2, 3], 'column2': [1, 3, 5]}\n",
    "    df = pd.DataFrame(data)\n",
    "    x, y, z = xrdos.split_and_scale(df, 1, (False, 1, False))\n",
    "    assert x[0] == 1\n",
    "    assert y.iloc[2].iloc[0] == 1\n",
    "    return\n",
    "\n",
    "def test_polynomialize():\n",
    "    data = {'column1': [2, 2, 3], 'column2': [1, 3, 5]}\n",
    "    df = pd.DataFrame(data)\n",
    "    yes = [True, 2, True]\n",
    "    poly = xrdos.polynomialize(df, yes)\n",
    "    print(type(poly))\n",
    "    assert poly[0,0] == 1\n",
    "    assert poly[2, 3] == 15\n",
    "    assert type(poly) == np.ndarray\n",
    "    return\n",
    "\n",
    "\n",
    "def test_train_model():\n",
    "    data = {'column1': [2, 2, 3], 'column2': [1, 3, 5]}\n",
    "    df = pd.DataFrame(data)\n",
    "    data1 = {'column1': [2.0, 2.0, 3.0], 'column2': [1.0, 3.0, 5.0]}\n",
    "    df1 = pd.DataFrame(data)\n",
    "    model, accuracy, scaler = xrdos.train_model(df, df1, xrdos.linear_regression(), 1, [False, 1, False])\n",
    "    a = np.array(df.iloc[0]).reshape(-1,1)\n",
    "    assert int(model.predict(scaler.transform(a))[0][0]) == 2\n",
    "    assert type(model) == sklearn.linear_model.base.LinearRegression\n",
    "    \n",
    "    \n",
    "def test_model_prediction():\n",
    "    data = {'column1': [2, 3, 4], 'column2': [1, 3, 5],'column3': [1, 5, 10] }\n",
    "    df = pd.DataFrame(data)\n",
    "    properties, predictors = xrdos.split(df, 1) \n",
    "    predictors = pd.DataFrame(predictors)\n",
    "    model = xrdos.linear_regression()\n",
    "    fitted_model = model.fit(predictors, properties)\n",
    "    scaler = preprocessing.MinMaxScaler() \n",
    "    scaler.fit(properties)\n",
    "    prediction, accuracy = xrdos.model_prediction(df, fitted_model, scaler, 1, [False, 1, True])\n",
    "    assert int(prediction[0][0]) == -2\n",
    "    return\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4483989793545256"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9766/9766 [==============================] - 0s 3us/step\n"
     ]
    }
   ],
   "source": [
    "test_predictions, test_error = model_prediction(df_test, model, scaler, 1, [False, 2, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.1597717, 1.9235963, 2.0795944, ..., 2.1379144, 2.232712 ,\n",
       "       2.275712 ], dtype=float32)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = [model2, error2, scaler2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear, linerror, linscaler = train_model(df_train, df_validation, linear_regression(), 1, [True, 2, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_predictions, lin_error = model_prediction(df_test, linear, linscaler, 1, [True, 2, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matminer.data_retrieval.retrieve_MP import MPDataRetrieval\n",
    "mpdr = MPDataRetrieval(api_key='x5He3oeSg1eCaIU4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "CompositionError",
     "evalue": "all is an invalid formula!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCompositionError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-7b87e13cb50c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# This statements obtains and stores the relevant data from MPD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# NOTE: Si was used as the criteria only for testing purposes. It will be changed later on\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mMPD_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmpdr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcriteria\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'all'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mproperties\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'xrd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'band_gap'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'efermi'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mextract_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMPD_data_row\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matminer\\data_retrieval\\retrieve_MP.py\u001b[0m in \u001b[0;36mget_dataframe\u001b[1;34m(self, criteria, properties, index_mpid, **kwargs)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \"\"\"\n\u001b[0;32m     55\u001b[0m         data = self.get_data(criteria=criteria, properties=properties,\n\u001b[1;32m---> 56\u001b[1;33m                              index_mpid=index_mpid, **kwargs)\n\u001b[0m\u001b[0;32m     57\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproperties\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         for prop in [\"dos\", \"phonon_dos\",\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matminer\\data_retrieval\\retrieve_MP.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self, criteria, properties, mp_decode, index_mpid)\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex_mpid\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"material_id\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproperties\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0mproperties\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"material_id\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmprester\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcriteria\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmp_decode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pymatgen\\ext\\matproj.py\u001b[0m in \u001b[0;36mquery\u001b[1;34m(self, criteria, properties, chunk_size, max_tries_per_chunk, mp_decode)\u001b[0m\n\u001b[0;32m    751\u001b[0m         \"\"\"\n\u001b[0;32m    752\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcriteria\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 753\u001b[1;33m             \u001b[0mcriteria\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_criteria\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcriteria\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    754\u001b[0m         payload = {\"criteria\": json.dumps(criteria),\n\u001b[0;32m    755\u001b[0m                    \"properties\": json.dumps(properties)}\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pymatgen\\ext\\matproj.py\u001b[0m in \u001b[0;36mparse_criteria\u001b[1;34m(criteria_string)\u001b[0m\n\u001b[0;32m   1272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1273\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoks\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1274\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mparse_tok\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1275\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"$or\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparse_tok\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pymatgen\\ext\\matproj.py\u001b[0m in \u001b[0;36mparse_tok\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1263\u001b[0m                 \u001b[0mparts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mparse_sym\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparts\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproduct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mparts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1265\u001b[1;33m                     \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mComposition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1266\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnelements\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m                         \u001b[1;31m# Check for valid Elements in keys.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pymatgen\\core\\composition.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[0melmap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m             \u001b[0melmap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_formula\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[0melmap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pymatgen\\core\\composition.py\u001b[0m in \u001b[0;36m_parse_formula\u001b[1;34m(self, formula)\u001b[0m\n\u001b[0;32m    540\u001b[0m             \u001b[0mexpanded_formula\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformula\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_sym\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_formula\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpanded_formula\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mget_sym_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformula\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pymatgen\\core\\composition.py\u001b[0m in \u001b[0;36mget_sym_dict\u001b[1;34m(f, factor)\u001b[0m\n\u001b[0;32m    527\u001b[0m                 \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mCompositionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{} is an invalid formula!\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msym_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCompositionError\u001b[0m: all is an invalid formula!"
     ]
    }
   ],
   "source": [
    "# This statements obtains and stores the relevant data from MPD\n",
    "# NOTE: Si was used as the criteria only for testing purposes. It will be changed later on\n",
    "MPD_data = mpdr.get_dataframe(criteria='all',properties=['xrd', 'band_gap', 'efermi'])\n",
    "\n",
    "def extract_data(MPD_data_row):\n",
    "    \"\"\"\n",
    "    Extracts the relevant XRD data from the dictionary obtained from MPD\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    MPD_data_row : Pandas dataframe\n",
    "         A row of data for a single material from the full MPD dataframe \n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    clean_df: Pandas dataframe\n",
    "        The top 10 XRD peaks and their corresponding two theta values for the material\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extracting out the amplitude and two theta values from the dictionary contained inside the received data\n",
    "    # then turning it into a pandas dataframe.\n",
    "    dirty_df = pd.DataFrame(MPD_data_row['xrd']['Cu']['pattern'], columns=MPD_data_row['xrd']['Cu']['meta']) # Converts data into dataframe\n",
    "    dirty_df.drop(['hkl','d_spacing'], axis=1, inplace=True) # Disposes of the hkl and d-spacing data\n",
    "\n",
    "    # Sorting the peaks into the top 10 with the highest peaks\n",
    "    dirty_df.sort_values('amplitude', ascending=False, inplace=True) # Sorts peaks from highest to smallest\n",
    "    dirty_df.reset_index(drop=True, inplace=True) # Reseting index\n",
    "    clean_df = dirty_df[:10] # Dropping all peaks below the top ten \n",
    "\n",
    "    return clean_df\n",
    "\n",
    "# Function to reformat the data after cleaning\n",
    "# Takes the dataframe and turns it into a dictionary wwhere all data points have a unique key\n",
    "def reformat_data(MPD_data_row):\n",
    "    \"\"\"\n",
    "    Reformats the cleaned data obtained from the extract_data function into a dictionary\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    MPD_data_row : Pandas dataframe\n",
    "         A row of data for a single material from the full MPD dataframe \n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    clean_df: Pandas dataframe\n",
    "        The top 10 XRD peaks and their corresponding two theta values for the material\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cleaning data and creating empty dictionary\n",
    "    clean_df = extract_data(MPD_data_row)\n",
    "    mat_dict = {}\n",
    "\n",
    "    # Loop to assign each data point to a key and stores it within the dictionary\n",
    "    for i in range(0,20):\n",
    "        if i < 10:\n",
    "            amp_key = ('amplitude_' + str(i))\n",
    "            mat_dict[amp_key] = clean_df['amplitude'][i]\n",
    "\n",
    "        else:\n",
    "            theta_key = ('two_theta_' + str(i-10))\n",
    "            mat_dict[theta_key] = clean_df['two_theta'][i-10]\n",
    "\n",
    "    return mat_dict\n",
    "\n",
    "# Function \n",
    "def produce_data(MPD_data):\n",
    "    \"\"\"\n",
    "    Produces the XRD and DOS data for all the materials passed to the function \n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    MPD_data : Pandas dataframe\n",
    "      The dataframe filled with data obtained from MPD \n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    full_df: Pandas dataframe\n",
    "        The peaks, two theta values, band gap, and fermi energy for all the materials passed to the function\n",
    "    \"\"\"\n",
    "    \n",
    "    # Creating prelimanry containers for XRD and DOS data\n",
    "    xrd_data = {}\n",
    "    dos_data = MPD_data.drop(['xrd'], axis=1)\n",
    "    \n",
    "    # Loop to run through each row of the dataframe\n",
    "    for i in range(len(MPD_data)):\n",
    "        \n",
    "        # Conditional to skip over materials with less than 10 XRD peaks\n",
    "        # or no fermi energies\n",
    "        if len(MPD_data.iloc[i]['xrd']['Cu']['pattern']) >= 10 and np.isnan(MPD_data.iloc[i]['efermi']) == False:\n",
    "            \n",
    "            # Obtaining and storing the XRD data for a material into a dictionary\n",
    "            ID = MPD_data.index[i]\n",
    "            mat_dict = reformat_data(MPD_data.iloc[i])\n",
    "            xrd_data[ID] = mat_dict\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # Replaces rows that failed the conditional with NaN\n",
    "            # This is for easy removal od the rows\n",
    "            dos_data.iloc[i] = float('nan')\n",
    "    \n",
    "    # Creating the final dataframe from the obtained XRD and DOS dataframes\n",
    "    dos_df = dos_data.dropna()\n",
    "    xrd_df = pd.DataFrame.from_dict(xrd_data, orient='index')\n",
    "    full_df = pd.concat([xrd_df, dos_df], axis=1, sort=False)\n",
    "    \n",
    "    return full_df\n",
    "\n",
    "produce_data(MPD_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure inputs are correct, thats where shit gets fucked up\n",
    "d_train = produce_data(MPD_data)\n",
    "model, val_error, scaler = train_model(d_train.iloc[0:12], d_train.iloc[12:25],linear_regression(), 2)\n",
    "predictions, accuracy = model_prediction(d_train.iloc[0:2], model, scaler, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0565    , 5.54577704],\n",
       "       [1.82      , 3.21901393]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7240086067659812e-15"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
