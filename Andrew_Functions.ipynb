{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, metrics \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from keras.models import Model, Sequential\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.layers import Input, Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: none of these fxns are meant for single point input, input should always be a pd.DataFrame with more\n",
    "#than one row, not an unreasonable request since sklearn makes you reshape 1D arrays.\n",
    "\n",
    "\n",
    "# Data cleaning section: \n",
    "\n",
    "def split_and_scale(df_train, n):\n",
    "    \"\"\"Splits training dataframe into predictors and properties to be predicted and returns them in 2 new dfs.\n",
    "       This function assumes all of the predictors are grouped together on the right side of the df.\n",
    "       df_train: training df\n",
    "       n: number of properties to be predicted(number of outputs)\"\"\"\n",
    "    properties, predictors = split(df_train, n)\n",
    "    # COMMENT OUT THIS LINE IF YOU DONT WANT TO HAVE POLYNOMIAL TERMS IN YOUR TRAINING DATA\n",
    "    # But note that accuracy is much better with this, but the model will have higher variance\n",
    "    predictors_polynomial = polynomialize(predictors)\n",
    "    predictors_scaled_polynomial, predictors_scaler_polynomial = scaling(predictors_polynomial)\n",
    "    return properties, predictors_scaled_polynomial, predictors_scaler_polynomial \n",
    "\n",
    "\n",
    "def polynomialize(series):\n",
    "    \"\"\"Adds polynomial features to degree 3, including interaction features. \n",
    "    series: an input ndarray of floats to be polynomialized.\n",
    "    This function returns a ndarray of all of the features specified above.\"\"\"\n",
    "    # Creating polynomial object\n",
    "    poly = PolynomialFeatures(degree = 3)\n",
    "    # Adding polynomial terms\n",
    "    predictors_polynomial = poly.fit_transform(series)\n",
    "    return predictors_polynomial\n",
    "\n",
    "# Still in development, in case we want to add more terms that aren't polynomial\n",
    "# def add_nonlinear_terms(df, n):\n",
    "#     properties = df[df.columns[-n:]]\n",
    "#     predictors = df[df.columns[:-n]]\n",
    "#     i = np.arange(len(predictors.columns) * 4)\n",
    "#     x = 0\n",
    "#     for column in predictors.values:\n",
    "#         predictors.assign(i[x]=column**2)\n",
    "#         predictors.assign(column**3)\n",
    "#         predictors.assign(np.exp(column))\n",
    "#         predictors.assign(np.sign(column))\n",
    "#     return properties, predictors\n",
    "\n",
    "\n",
    "def split(df, n):\n",
    "    \"\"\"Takes an input pd.DataFrame and returns 2 ndarrays of the properties \n",
    "    and predictors.\"\"\"\n",
    "    properties = df[df.columns[-n:]].values\n",
    "    predictors = df[df.columns[:-n]].values\n",
    "    return properties, predictors\n",
    "\n",
    "\n",
    "def scaling(df_train):\n",
    "    \"\"\"This function takes a pd.DataFrame, creates a sklearn.StandardScaler, scales the DataFrame,\n",
    "       and returns the scaled data in a pd.DataFrame as well as the sklearn.StandardScaler object\n",
    "       for transforming data back to unscaled form post machine learning.\n",
    "       df_train: pd.DataFrame(for our purposes should be of shape 20 columns by an arbitrary number of rows)\"\"\"\n",
    "    #Creating scaler object\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    #Scaling df_train\n",
    "    scaled_data = pd.DataFrame(scaler.fit_transform(df_train))\n",
    "    \n",
    "    return scaled_data, scaler\n",
    "\n",
    "# Training/predicting\n",
    "\n",
    "\n",
    "def train_model(df_train, df_validation, model, n):\n",
    "    \"\"\"This function takes a training DataFrame, validation DataFrame and a preconfigured model\n",
    "       and trains said model on the training data followed by measuring error on validation data and \n",
    "       returning both the trained model and accuracy metric. This function assumes whatever parameter(s)\n",
    "       being predicted is in the last column(s) of df_train.\n",
    "       n: number of outputs\n",
    "       because this function returns the trained model, more metrics can be performed later that are specific\n",
    "       to whatever package it is in/the type of model it is\n",
    "       Parameters\"\"\"\n",
    "    #generating scaled data and their respective scaler objects\n",
    "    t_properties, t_predictors_scaled, t_predictors_scaler = split_and_scale(df_train, n)\n",
    "    v_properties, v_predictors_scaled, v_predictors_scaler = split_and_scale(df_validation, n)\n",
    "    #supervised learning of predictors and properties to fit model, note: keras does not take pd.DataFrames for\n",
    "    #training, using .values fixes this\n",
    "    model.fit(t_predictors_scaled, t_properties)\n",
    "    #predicting output of validation set\n",
    "    predictions = pd.DataFrame(model.predict(v_predictors_scaled))\n",
    "    #calculating RMSE from sklearn package\n",
    "    val_error = np.sqrt(metrics.mean_squared_error(predictions, v_properties))\n",
    "    return model, val_error, t_predictors_scaler\n",
    "\n",
    "\n",
    "def model_prediction(test_data, fitted_model, scaler, n):\n",
    "    \"\"\"Takes a fitted model and predicts the output of test data, returns the predicted data and accuracy.\n",
    "       THIS FUNCTION IS ONLY TO BE USED FOR FUTURE PREDICTIONS OR TESTING(WHICH SHOULD ONLY BE DONE ONCE).\n",
    "       Do not use this while training a model, that's what the validation data will be used for. We do not \n",
    "       want to introduce bias into our model by fitting to the test data\n",
    "       n = number of predictors\"\"\"\n",
    "    \n",
    "    #splitting predictors and properties\n",
    "    properties, predictors = split(test_data, n)\n",
    "    predictors = polynomialize(predictors)\n",
    "    predictors_scaled = scaler.transform(predictors)\n",
    "    #predicting based on scaled input predictors\n",
    "    prediction = fitted_model.predict(predictors_scaled)\n",
    "    #calculating MSE\n",
    "    accuracy_metric = np.sqrt(metrics.mean_squared_error(properties, prediction))\n",
    "\n",
    "    return prediction, accuracy_metric\n",
    "\n",
    "# Below functions initialize all the different types of models we are looking at:\n",
    "\n",
    "\n",
    "def neural_network():\n",
    "    \"\"\"Creates a neural network object to be passed into train_model function, can change properties of net\n",
    "       here.\"\"\"\n",
    "    def model():\n",
    "        model = Sequential()\n",
    "        model.add(Dense(84, input_dim=84, kernel_initializer='normal', activation='relu'))\n",
    "        model.add(Dense(84, kernel_initializer='normal', activation='relu'))\n",
    "        model.add(Dense(1, kernel_initializer = 'normal'))#kernel_initializer = initial values of outputs i think\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        return model\n",
    "    network = KerasRegressor(build_fn=model, epochs=100, batch_size=25000, verbose=0)\n",
    "#     network.fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, \n",
    "    return network\n",
    "\n",
    "\n",
    "def linear_regression():\n",
    "    \"\"\"creates a linear regression object\"\"\"\n",
    "    regr = LinearRegression()\n",
    "    return regr\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "def function_to_train_model(df_train, df_validation, model_type):\n",
    "\n",
    "blah blah\n",
    "train the (model_type) algorithm on X_train,\n",
    "test the (model_type) on X_validation\n",
    "measure the performance\n",
    "\n",
    "return fitted_model_type, accuracy_metric (e.g., RMSE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('HCEPD_100K.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data[['e_homo_alpha', 'jsc', 'e_gap_alpha', 'e_lumo_alpha', 'mass','voc', 'pce']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression test - accuracy is 100x better after adding polynomial terms\n",
    "model, val_error, scaler = train_model(sample.iloc[:80000], sample.iloc[80000:],linear_regression(), 1)\n",
    "model_prediction(sample.iloc[0:5], model, scaler, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network test, make sure to update your layer for expected input\n",
    "model1, val_error1, scaler1 = train_model(sample.iloc[:80000], sample.iloc[80000:],neural_network(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prediction(sample.iloc[0:5], model1, scaler1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO REDUCE OVERFITTING: reduce degree of polynomial terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model might just not have the right things\n",
    "#to reorder columns:\n",
    "#cols = df.columns.tolist()\n",
    "#df = df[cols] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
