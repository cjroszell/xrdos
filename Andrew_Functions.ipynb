{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import preprocessing, metrics \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from keras.models import Model, Sequential\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras import optimizers\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: none of these fxns are meant for single point input, input should always be a pd.DataFrame with more\n",
    "#than one row, not an unreasonable request since sklearn makes you reshape 1D arrays.\n",
    "\n",
    "\n",
    "# Data cleaning section: \n",
    "\n",
    "def split_and_scale(df, n, yes):\n",
    "    \"\"\"Splits training dataframe into predictors and properties to be predicted and returns them in 2 new dfs.\n",
    "       This function assumes all of the predictors are grouped together on the right side of the df.\n",
    "       df_train: training df\n",
    "       n: number of properties to be predicted(number of outputs)\"\"\"\n",
    "    properties, predictors = split(df, n)\n",
    "    # COMMENT OUT THIS LINE IF YOU DONT WANT TO HAVE POLYNOMIAL TERMS IN YOUR TRAINING DATA\n",
    "    # But note that accuracy is much better with this, but the model will have higher variance\n",
    "    predictors_polynomial = polynomialize(predictors, yes)\n",
    "    predictors_scaled_polynomial, predictors_scaler_polynomial = scaling(predictors_polynomial)\n",
    "    return properties, predictors_scaled_polynomial, predictors_scaler_polynomial \n",
    "\n",
    "\n",
    "def polynomialize(series, yes):\n",
    "    \"\"\"Adds polynomial features to degree 3, including interaction features. \n",
    "    series: an input ndarray of floats to be polynomialized.\n",
    "    This function returns a ndarray of all of the features specified above.\"\"\"\n",
    "    # Creating polynomial object\n",
    "    if yes[0]:\n",
    "        poly = PolynomialFeatures(degree = yes[1])\n",
    "        # Adding polynomial terms\n",
    "        series = poly.fit_transform(series)\n",
    "    return series\n",
    "\n",
    "# Still in development, in case we want to add more terms that aren't polynomial\n",
    "# def add_nonlinear_terms(df, n):\n",
    "#     properties = df[df.columns[-n:]]\n",
    "#     predictors = df[df.columns[:-n]]\n",
    "#     i = np.arange(len(predictors.columns) * 4)\n",
    "#     x = 0\n",
    "#     for column in predictors.values:\n",
    "#         predictors.assign(i[x]=column**2)\n",
    "#         predictors.assign(column**3)\n",
    "#         predictors.assign(np.exp(column))\n",
    "#         predictors.assign(np.sign(column))\n",
    "#     return properties, predictors\n",
    "\n",
    "\n",
    "def split(df, n):\n",
    "    \"\"\"Takes an input pd.DataFrame and returns 2 ndarrays of the properties \n",
    "    and predictors.\"\"\"\n",
    "    properties = df[df.columns[-n:]].values\n",
    "    predictors = df[df.columns[:-n]].values\n",
    "    return properties, predictors\n",
    "\n",
    "\n",
    "def scaling(df_train):\n",
    "    \"\"\"This function takes a pd.DataFrame, creates a sklearn.StandardScaler, scales the DataFrame,\n",
    "       and returns the scaled data in a pd.DataFrame as well as the sklearn.StandardScaler object\n",
    "       for transforming data back to unscaled form post machine learning.\n",
    "       df_train: pd.DataFrame(for our purposes should be of shape 20 columns by an arbitrary number of rows)\"\"\"\n",
    "    #Creating scaler object\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    #Scaling df_train\n",
    "    scaled_data = pd.DataFrame(scaler.fit_transform(df_train))\n",
    "    \n",
    "    return scaled_data, scaler\n",
    "\n",
    "# Training/predicting\n",
    "\n",
    "\n",
    "def train_model(df_train, df_validation, model, n, yes):\n",
    "    \"\"\"This function takes a training DataFrame, validation DataFrame and a preconfigured model\n",
    "       and trains said model on the training data followed by measuring error on validation data and \n",
    "       returning both the trained model and accuracy metric. This function assumes whatever parameter(s)\n",
    "       being predicted is in the last column(s) of df_train.\n",
    "       n: number of outputs\n",
    "       because this function returns the trained model, more metrics can be performed later that are specific\n",
    "       to whatever package it is in/the type of model it is\n",
    "       Parameters\"\"\"\n",
    "    #generating scaled data and their respective scaler objects\n",
    "    t_properties, t_predictors_scaled, t_predictors_scaler = split_and_scale(df_train, n, yes)\n",
    "    v_properties, v_predictors_scaled, v_predictors_scaler = split_and_scale(df_validation, n, yes)\n",
    "    #supervised learning of predictors and properties to fit model, note: keras does not take pd.DataFrames for\n",
    "    #training, using .values fixes this\n",
    "    model.fit(t_predictors_scaled, t_properties)\n",
    "    #predicting output of validation set\n",
    "    predictions = pd.DataFrame(model.predict(v_predictors_scaled))\n",
    "    #calculating RMSE from sklearn package\n",
    "    val_error = np.sqrt(metrics.mean_squared_error(predictions, v_properties))\n",
    "    return model, val_error, t_predictors_scaler\n",
    "\n",
    "\n",
    "def model_prediction(test_data, fitted_model, scaler, n):\n",
    "    \"\"\"Takes a fitted model and predicts the output of test data, returns the predicted data and accuracy.\n",
    "       THIS FUNCTION IS ONLY TO BE USED FOR FUTURE PREDICTIONS OR TESTING(WHICH SHOULD ONLY BE DONE ONCE).\n",
    "       Do not use this while training a model, that's what the validation data will be used for. We do not \n",
    "       want to introduce bias into our model by fitting to the test data\n",
    "       n = number of predictors\"\"\"\n",
    "    #splitting predictors and properties\n",
    "    properties, predictors = split(test_data, n)\n",
    "    predictors = polynomialize(predictors)\n",
    "    predictors_scaled = scaler.transform(predictors)\n",
    "    #predicting based on scaled input predictors\n",
    "    prediction = fitted_model.predict(predictors_scaled)\n",
    "    #calculating MSE\n",
    "    accuracy_metric = np.sqrt(metrics.mean_squared_error(properties, prediction))\n",
    "\n",
    "    return prediction, accuracy_metric\n",
    "\n",
    "# Below functions initialize all the different types of models we are looking at:\n",
    "\n",
    "\n",
    "def neural_network():\n",
    "    \"\"\"Creates a neural network object to be passed into train_model function, can change properties of net\n",
    "       here.\"\"\"\n",
    "    def model():\n",
    "        model = Sequential()\n",
    "        model.add(Dense(50, input_dim=20, kernel_initializer='normal', activation='relu'))\n",
    "        model.add(Dense(50, kernel_initializer='normal', activation='relu'))\n",
    "        model.add(Dense(20, kernel_initializer='normal', activation='relu'))\n",
    "        model.add(Dense(1, kernel_initializer = 'normal'))#kernel_initializer = initial values of outputs i think\n",
    "        model.compile(optimizer=optimizers.Adam(lr=1.0e-4), loss='mse', metrics = ['accuracy'])\n",
    "        return model\n",
    "    network = KerasRegressor(build_fn=model, epochs=150, batch_size=50, verbose=1)\n",
    "#     network.fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, \n",
    "    return network\n",
    "\n",
    "\n",
    "def linear_regression():\n",
    "    \"\"\"creates a linear regression object\"\"\"\n",
    "    regr = LinearRegression()\n",
    "    return regr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.linear_model.base.LinearRegression"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to go in cleaning section\n",
    "# import clean\n",
    "\n",
    "def test_split():\n",
    "    data = {'column1': [2, 2, 3], 'column2': [1, 3, 5]}\n",
    "    df = pd.DataFrame(data)\n",
    "    one, two = clean.split(df, 1)\n",
    "    assert one[0] == 1\n",
    "    assert two[0] == 2\n",
    "    return\n",
    "\n",
    "def test_scaling():\n",
    "    data = {'column1': [2.0, 2.0, 3.0], 'column2': [1.0, 3.0, 5.0]}\n",
    "    df = pd.DataFrame(data)\n",
    "    df, scaler = clean.scaling(df)\n",
    "    assert df.loc[0].iloc[0] == 0\n",
    "    assert df.loc[2].iloc[0] == 1\n",
    "    return\n",
    "\n",
    "# def test_linear_regression():\n",
    "#     return\n",
    "\n",
    "regr = LinearRegression()\n",
    "type(regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df): \n",
    "    train, test_and_val = sklearn.model_selection.train_test_split(df, test_size=.30)\n",
    "    test, val = sklearn.model_selection.train_test_split(test_and_val, test_size=.15)\n",
    "    df_train = pd.DataFrame(train)\n",
    "    df_val = pd.DataFrame(val)\n",
    "    df_test = pd.DataFrame(test)\n",
    "    df_train.drop(columns = 'efermi',inplace = True)\n",
    "    df_val.drop(columns = 'efermi',inplace = True)\n",
    "    df_test.drop(columns = 'efermi',inplace = True) \n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amplitude_0</th>\n",
       "      <th>amplitude_1</th>\n",
       "      <th>amplitude_2</th>\n",
       "      <th>amplitude_3</th>\n",
       "      <th>amplitude_4</th>\n",
       "      <th>amplitude_5</th>\n",
       "      <th>amplitude_6</th>\n",
       "      <th>amplitude_7</th>\n",
       "      <th>amplitude_8</th>\n",
       "      <th>amplitude_9</th>\n",
       "      <th>...</th>\n",
       "      <th>two_theta_2</th>\n",
       "      <th>two_theta_3</th>\n",
       "      <th>two_theta_4</th>\n",
       "      <th>two_theta_5</th>\n",
       "      <th>two_theta_6</th>\n",
       "      <th>two_theta_7</th>\n",
       "      <th>two_theta_8</th>\n",
       "      <th>two_theta_9</th>\n",
       "      <th>band_gap</th>\n",
       "      <th>efermi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mp-1000</th>\n",
       "      <td>100.0</td>\n",
       "      <td>76.808467</td>\n",
       "      <td>60.517635</td>\n",
       "      <td>29.361717</td>\n",
       "      <td>22.132998</td>\n",
       "      <td>21.657248</td>\n",
       "      <td>18.275875</td>\n",
       "      <td>17.467588</td>\n",
       "      <td>15.621411</td>\n",
       "      <td>15.480452</td>\n",
       "      <td>...</td>\n",
       "      <td>35.823897</td>\n",
       "      <td>58.193688</td>\n",
       "      <td>44.255935</td>\n",
       "      <td>64.376264</td>\n",
       "      <td>127.447337</td>\n",
       "      <td>153.100605</td>\n",
       "      <td>134.637754</td>\n",
       "      <td>108.920846</td>\n",
       "      <td>1.5930</td>\n",
       "      <td>2.087033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mp-10009</th>\n",
       "      <td>100.0</td>\n",
       "      <td>78.502945</td>\n",
       "      <td>61.382764</td>\n",
       "      <td>59.581084</td>\n",
       "      <td>51.834414</td>\n",
       "      <td>42.578817</td>\n",
       "      <td>34.238297</td>\n",
       "      <td>32.474708</td>\n",
       "      <td>26.611831</td>\n",
       "      <td>24.362379</td>\n",
       "      <td>...</td>\n",
       "      <td>19.268570</td>\n",
       "      <td>24.866325</td>\n",
       "      <td>43.790584</td>\n",
       "      <td>158.910531</td>\n",
       "      <td>161.241471</td>\n",
       "      <td>48.251965</td>\n",
       "      <td>31.649554</td>\n",
       "      <td>62.766975</td>\n",
       "      <td>0.7804</td>\n",
       "      <td>2.669606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mp-1001012</th>\n",
       "      <td>100.0</td>\n",
       "      <td>80.540257</td>\n",
       "      <td>76.206648</td>\n",
       "      <td>35.729962</td>\n",
       "      <td>34.640541</td>\n",
       "      <td>30.607739</td>\n",
       "      <td>26.076216</td>\n",
       "      <td>23.924735</td>\n",
       "      <td>18.361390</td>\n",
       "      <td>15.331991</td>\n",
       "      <td>...</td>\n",
       "      <td>48.832011</td>\n",
       "      <td>23.855655</td>\n",
       "      <td>44.629498</td>\n",
       "      <td>33.990068</td>\n",
       "      <td>135.125592</td>\n",
       "      <td>91.443607</td>\n",
       "      <td>145.702725</td>\n",
       "      <td>41.952291</td>\n",
       "      <td>0.5765</td>\n",
       "      <td>3.071523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mp-1001015</th>\n",
       "      <td>100.0</td>\n",
       "      <td>77.813956</td>\n",
       "      <td>60.044813</td>\n",
       "      <td>44.937326</td>\n",
       "      <td>38.637906</td>\n",
       "      <td>25.032308</td>\n",
       "      <td>22.305078</td>\n",
       "      <td>21.849105</td>\n",
       "      <td>19.221977</td>\n",
       "      <td>16.104948</td>\n",
       "      <td>...</td>\n",
       "      <td>169.262733</td>\n",
       "      <td>32.462334</td>\n",
       "      <td>42.581227</td>\n",
       "      <td>86.418577</td>\n",
       "      <td>64.924776</td>\n",
       "      <td>124.232687</td>\n",
       "      <td>22.798656</td>\n",
       "      <td>132.065583</td>\n",
       "      <td>0.6698</td>\n",
       "      <td>2.212315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mp-1001016</th>\n",
       "      <td>100.0</td>\n",
       "      <td>70.020649</td>\n",
       "      <td>63.037973</td>\n",
       "      <td>48.338026</td>\n",
       "      <td>34.510549</td>\n",
       "      <td>31.212189</td>\n",
       "      <td>27.679137</td>\n",
       "      <td>26.613327</td>\n",
       "      <td>24.481458</td>\n",
       "      <td>20.327996</td>\n",
       "      <td>...</td>\n",
       "      <td>27.919942</td>\n",
       "      <td>168.168263</td>\n",
       "      <td>26.708785</td>\n",
       "      <td>86.053690</td>\n",
       "      <td>55.025631</td>\n",
       "      <td>165.708924</td>\n",
       "      <td>123.501849</td>\n",
       "      <td>77.054857</td>\n",
       "      <td>0.1396</td>\n",
       "      <td>2.638915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            amplitude_0  amplitude_1  amplitude_2  amplitude_3  amplitude_4  \\\n",
       "mp-1000           100.0    76.808467    60.517635    29.361717    22.132998   \n",
       "mp-10009          100.0    78.502945    61.382764    59.581084    51.834414   \n",
       "mp-1001012        100.0    80.540257    76.206648    35.729962    34.640541   \n",
       "mp-1001015        100.0    77.813956    60.044813    44.937326    38.637906   \n",
       "mp-1001016        100.0    70.020649    63.037973    48.338026    34.510549   \n",
       "\n",
       "            amplitude_5  amplitude_6  amplitude_7  amplitude_8  amplitude_9  \\\n",
       "mp-1000       21.657248    18.275875    17.467588    15.621411    15.480452   \n",
       "mp-10009      42.578817    34.238297    32.474708    26.611831    24.362379   \n",
       "mp-1001012    30.607739    26.076216    23.924735    18.361390    15.331991   \n",
       "mp-1001015    25.032308    22.305078    21.849105    19.221977    16.104948   \n",
       "mp-1001016    31.212189    27.679137    26.613327    24.481458    20.327996   \n",
       "\n",
       "            ...  two_theta_2  two_theta_3  two_theta_4  two_theta_5  \\\n",
       "mp-1000     ...    35.823897    58.193688    44.255935    64.376264   \n",
       "mp-10009    ...    19.268570    24.866325    43.790584   158.910531   \n",
       "mp-1001012  ...    48.832011    23.855655    44.629498    33.990068   \n",
       "mp-1001015  ...   169.262733    32.462334    42.581227    86.418577   \n",
       "mp-1001016  ...    27.919942   168.168263    26.708785    86.053690   \n",
       "\n",
       "            two_theta_6  two_theta_7  two_theta_8  two_theta_9  band_gap  \\\n",
       "mp-1000      127.447337   153.100605   134.637754   108.920846    1.5930   \n",
       "mp-10009     161.241471    48.251965    31.649554    62.766975    0.7804   \n",
       "mp-1001012   135.125592    91.443607   145.702725    41.952291    0.5765   \n",
       "mp-1001015    64.924776   124.232687    22.798656   132.065583    0.6698   \n",
       "mp-1001016    55.025631   165.708924   123.501849    77.054857    0.1396   \n",
       "\n",
       "              efermi  \n",
       "mp-1000     2.087033  \n",
       "mp-10009    2.669606  \n",
       "mp-1001012  3.071523  \n",
       "mp-1001015  2.212315  \n",
       "mp-1001016  2.638915  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('MPD_Data_Processed.csv', sep = '\\t', index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amplitude_0</th>\n",
       "      <th>amplitude_1</th>\n",
       "      <th>amplitude_2</th>\n",
       "      <th>amplitude_3</th>\n",
       "      <th>amplitude_4</th>\n",
       "      <th>amplitude_5</th>\n",
       "      <th>amplitude_6</th>\n",
       "      <th>amplitude_7</th>\n",
       "      <th>amplitude_8</th>\n",
       "      <th>amplitude_9</th>\n",
       "      <th>...</th>\n",
       "      <th>two_theta_1</th>\n",
       "      <th>two_theta_2</th>\n",
       "      <th>two_theta_3</th>\n",
       "      <th>two_theta_4</th>\n",
       "      <th>two_theta_5</th>\n",
       "      <th>two_theta_6</th>\n",
       "      <th>two_theta_7</th>\n",
       "      <th>two_theta_8</th>\n",
       "      <th>two_theta_9</th>\n",
       "      <th>band_gap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mp-651008</th>\n",
       "      <td>100.0</td>\n",
       "      <td>78.877404</td>\n",
       "      <td>69.497273</td>\n",
       "      <td>62.020021</td>\n",
       "      <td>54.668569</td>\n",
       "      <td>52.144988</td>\n",
       "      <td>49.189664</td>\n",
       "      <td>48.272557</td>\n",
       "      <td>47.844256</td>\n",
       "      <td>41.007436</td>\n",
       "      <td>...</td>\n",
       "      <td>23.659384</td>\n",
       "      <td>18.810940</td>\n",
       "      <td>13.402824</td>\n",
       "      <td>27.416565</td>\n",
       "      <td>33.367866</td>\n",
       "      <td>31.209016</td>\n",
       "      <td>31.849605</td>\n",
       "      <td>26.993525</td>\n",
       "      <td>30.830455</td>\n",
       "      <td>0.3275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mp-1112544</th>\n",
       "      <td>100.0</td>\n",
       "      <td>72.609265</td>\n",
       "      <td>62.918017</td>\n",
       "      <td>37.510808</td>\n",
       "      <td>29.959400</td>\n",
       "      <td>26.651730</td>\n",
       "      <td>25.835442</td>\n",
       "      <td>25.555973</td>\n",
       "      <td>24.884538</td>\n",
       "      <td>24.666135</td>\n",
       "      <td>...</td>\n",
       "      <td>41.119585</td>\n",
       "      <td>24.837604</td>\n",
       "      <td>175.147257</td>\n",
       "      <td>67.458867</td>\n",
       "      <td>159.187804</td>\n",
       "      <td>20.226261</td>\n",
       "      <td>48.635985</td>\n",
       "      <td>50.949291</td>\n",
       "      <td>136.604576</td>\n",
       "      <td>1.0046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mp-556334</th>\n",
       "      <td>100.0</td>\n",
       "      <td>74.303418</td>\n",
       "      <td>73.613068</td>\n",
       "      <td>51.507414</td>\n",
       "      <td>50.961419</td>\n",
       "      <td>43.982730</td>\n",
       "      <td>38.078149</td>\n",
       "      <td>32.232925</td>\n",
       "      <td>30.398982</td>\n",
       "      <td>28.006856</td>\n",
       "      <td>...</td>\n",
       "      <td>31.622812</td>\n",
       "      <td>32.957886</td>\n",
       "      <td>31.565976</td>\n",
       "      <td>25.085241</td>\n",
       "      <td>33.121726</td>\n",
       "      <td>18.913250</td>\n",
       "      <td>21.185866</td>\n",
       "      <td>46.405540</td>\n",
       "      <td>39.465870</td>\n",
       "      <td>3.5264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mp-540341</th>\n",
       "      <td>100.0</td>\n",
       "      <td>98.579284</td>\n",
       "      <td>68.292901</td>\n",
       "      <td>53.313382</td>\n",
       "      <td>47.230059</td>\n",
       "      <td>46.441547</td>\n",
       "      <td>43.303820</td>\n",
       "      <td>30.677619</td>\n",
       "      <td>30.473473</td>\n",
       "      <td>26.286890</td>\n",
       "      <td>...</td>\n",
       "      <td>30.515703</td>\n",
       "      <td>18.918462</td>\n",
       "      <td>31.756397</td>\n",
       "      <td>33.558030</td>\n",
       "      <td>27.458450</td>\n",
       "      <td>13.630336</td>\n",
       "      <td>47.053775</td>\n",
       "      <td>41.510396</td>\n",
       "      <td>37.875211</td>\n",
       "      <td>1.2423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mp-24428</th>\n",
       "      <td>100.0</td>\n",
       "      <td>36.204012</td>\n",
       "      <td>22.793257</td>\n",
       "      <td>22.325667</td>\n",
       "      <td>22.166651</td>\n",
       "      <td>18.716592</td>\n",
       "      <td>18.243829</td>\n",
       "      <td>17.294773</td>\n",
       "      <td>15.764271</td>\n",
       "      <td>15.758957</td>\n",
       "      <td>...</td>\n",
       "      <td>28.542301</td>\n",
       "      <td>27.335451</td>\n",
       "      <td>43.498485</td>\n",
       "      <td>39.131744</td>\n",
       "      <td>37.138187</td>\n",
       "      <td>40.815408</td>\n",
       "      <td>33.059746</td>\n",
       "      <td>25.084479</td>\n",
       "      <td>14.160052</td>\n",
       "      <td>2.0569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            amplitude_0  amplitude_1  amplitude_2  amplitude_3  amplitude_4  \\\n",
       "mp-651008         100.0    78.877404    69.497273    62.020021    54.668569   \n",
       "mp-1112544        100.0    72.609265    62.918017    37.510808    29.959400   \n",
       "mp-556334         100.0    74.303418    73.613068    51.507414    50.961419   \n",
       "mp-540341         100.0    98.579284    68.292901    53.313382    47.230059   \n",
       "mp-24428          100.0    36.204012    22.793257    22.325667    22.166651   \n",
       "\n",
       "            amplitude_5  amplitude_6  amplitude_7  amplitude_8  amplitude_9  \\\n",
       "mp-651008     52.144988    49.189664    48.272557    47.844256    41.007436   \n",
       "mp-1112544    26.651730    25.835442    25.555973    24.884538    24.666135   \n",
       "mp-556334     43.982730    38.078149    32.232925    30.398982    28.006856   \n",
       "mp-540341     46.441547    43.303820    30.677619    30.473473    26.286890   \n",
       "mp-24428      18.716592    18.243829    17.294773    15.764271    15.758957   \n",
       "\n",
       "            ...  two_theta_1  two_theta_2  two_theta_3  two_theta_4  \\\n",
       "mp-651008   ...    23.659384    18.810940    13.402824    27.416565   \n",
       "mp-1112544  ...    41.119585    24.837604   175.147257    67.458867   \n",
       "mp-556334   ...    31.622812    32.957886    31.565976    25.085241   \n",
       "mp-540341   ...    30.515703    18.918462    31.756397    33.558030   \n",
       "mp-24428    ...    28.542301    27.335451    43.498485    39.131744   \n",
       "\n",
       "            two_theta_5  two_theta_6  two_theta_7  two_theta_8  two_theta_9  \\\n",
       "mp-651008     33.367866    31.209016    31.849605    26.993525    30.830455   \n",
       "mp-1112544   159.187804    20.226261    48.635985    50.949291   136.604576   \n",
       "mp-556334     33.121726    18.913250    21.185866    46.405540    39.465870   \n",
       "mp-540341     27.458450    13.630336    47.053775    41.510396    37.875211   \n",
       "mp-24428      37.138187    40.815408    33.059746    25.084479    14.160052   \n",
       "\n",
       "            band_gap  \n",
       "mp-651008     0.3275  \n",
       "mp-1112544    1.0046  \n",
       "mp-556334     3.5264  \n",
       "mp-540341     1.2423  \n",
       "mp-24428      2.0569  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, df_validation, df_test = train_test_split(data)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "26810/26810 [==============================] - 2s 73us/step - loss: 3.9461 - acc: 7.4599e-05\n",
      "Epoch 2/150\n",
      "26810/26810 [==============================] - 1s 37us/step - loss: 2.3090 - acc: 3.7300e-05\n",
      "Epoch 3/150\n",
      "26810/26810 [==============================] - 1s 35us/step - loss: 2.2584 - acc: 7.4599e-05\n",
      "Epoch 4/150\n",
      "26810/26810 [==============================] - 1s 34us/step - loss: 2.2113 - acc: 3.7300e-05\n",
      "Epoch 5/150\n",
      "26810/26810 [==============================] - 1s 30us/step - loss: 2.1767 - acc: 3.7300e-05\n",
      "Epoch 6/150\n",
      "26810/26810 [==============================] - 1s 30us/step - loss: 2.1538 - acc: 3.7300e-05\n",
      "Epoch 7/150\n",
      "26810/26810 [==============================] - 1s 29us/step - loss: 2.1412 - acc: 3.7300e-05\n",
      "Epoch 8/150\n",
      "26810/26810 [==============================] - 1s 29us/step - loss: 2.1352 - acc: 3.7300e-05\n",
      "Epoch 9/150\n",
      "26810/26810 [==============================] - 1s 29us/step - loss: 2.1328 - acc: 3.7300e-05\n",
      "Epoch 10/150\n",
      "26810/26810 [==============================] - 1s 30us/step - loss: 2.1283 - acc: 3.7300e-05\n",
      "Epoch 11/150\n",
      "26810/26810 [==============================] - 1s 30us/step - loss: 2.1245 - acc: 3.7300e-05\n",
      "Epoch 12/150\n",
      "26810/26810 [==============================] - 1s 30us/step - loss: 2.1203 - acc: 3.7300e-05\n",
      "Epoch 13/150\n",
      "26810/26810 [==============================] - 1s 29us/step - loss: 2.1170 - acc: 3.7300e-05\n",
      "Epoch 14/150\n",
      "26810/26810 [==============================] - 1s 30us/step - loss: 2.1127 - acc: 3.7300e-05\n",
      "Epoch 15/150\n",
      "26810/26810 [==============================] - 1s 29us/step - loss: 2.1094 - acc: 3.7300e-05\n",
      "Epoch 16/150\n",
      "26810/26810 [==============================] - 1s 29us/step - loss: 2.1059 - acc: 3.7300e-05\n",
      "Epoch 17/150\n",
      "26810/26810 [==============================] - 1s 29us/step - loss: 2.1036 - acc: 3.7300e-05\n",
      "Epoch 18/150\n",
      "26810/26810 [==============================] - 1s 30us/step - loss: 2.0999 - acc: 3.7300e-05\n",
      "Epoch 19/150\n",
      "26810/26810 [==============================] - 1s 30us/step - loss: 2.0975 - acc: 3.7300e-05\n",
      "Epoch 20/150\n",
      "26810/26810 [==============================] - 1s 30us/step - loss: 2.0948 - acc: 3.7300e-05\n",
      "Epoch 21/150\n",
      "26810/26810 [==============================] - 1s 29us/step - loss: 2.0933 - acc: 3.7300e-05\n",
      "Epoch 22/150\n",
      "26810/26810 [==============================] - 1s 29us/step - loss: 2.0898 - acc: 3.7300e-05\n",
      "Epoch 23/150\n",
      "26810/26810 [==============================] - 1s 29us/step - loss: 2.0880 - acc: 3.7300e-05\n",
      "Epoch 24/150\n",
      "26810/26810 [==============================] - 1s 31us/step - loss: 2.0863 - acc: 3.7300e-05\n",
      "Epoch 25/150\n",
      "26810/26810 [==============================] - 1s 34us/step - loss: 2.0851 - acc: 3.7300e-05\n",
      "Epoch 26/150\n",
      "26810/26810 [==============================] - 1s 35us/step - loss: 2.0838 - acc: 3.7300e-05\n",
      "Epoch 27/150\n",
      "26810/26810 [==============================] - 1s 35us/step - loss: 2.0820 - acc: 3.7300e-05\n",
      "Epoch 28/150\n",
      "26810/26810 [==============================] - 1s 35us/step - loss: 2.0804 - acc: 3.7300e-05\n",
      "Epoch 29/150\n",
      "26810/26810 [==============================] - 1s 36us/step - loss: 2.0782 - acc: 3.7300e-05\n",
      "Epoch 30/150\n",
      "26810/26810 [==============================] - 1s 36us/step - loss: 2.0777 - acc: 3.7300e-05\n",
      "Epoch 31/150\n",
      "26810/26810 [==============================] - 1s 35us/step - loss: 2.0765 - acc: 3.7300e-05\n",
      "Epoch 32/150\n",
      "26810/26810 [==============================] - 1s 33us/step - loss: 2.0758 - acc: 3.7300e-05\n",
      "Epoch 33/150\n",
      "26810/26810 [==============================] - 1s 27us/step - loss: 2.0745 - acc: 3.7300e-05\n",
      "Epoch 34/150\n",
      "26810/26810 [==============================] - 1s 28us/step - loss: 2.0727 - acc: 3.7300e-05\n",
      "Epoch 35/150\n",
      "26810/26810 [==============================] - 1s 27us/step - loss: 2.0728 - acc: 3.7300e-05\n",
      "Epoch 36/150\n",
      "26810/26810 [==============================] - 1s 29us/step - loss: 2.0716 - acc: 3.7300e-05\n",
      "Epoch 37/150\n",
      "26810/26810 [==============================] - 1s 29us/step - loss: 2.0711 - acc: 3.7300e-05\n",
      "Epoch 38/150\n",
      "26810/26810 [==============================] - 1s 28us/step - loss: 2.0704 - acc: 3.7300e-05\n",
      "Epoch 39/150\n",
      "26810/26810 [==============================] - 1s 29us/step - loss: 2.0694 - acc: 3.7300e-05\n",
      "Epoch 40/150\n",
      "26810/26810 [==============================] - 1s 27us/step - loss: 2.0675 - acc: 3.7300e-05\n",
      "Epoch 41/150\n",
      "26810/26810 [==============================] - 1s 28us/step - loss: 2.0658 - acc: 3.7300e-05\n",
      "Epoch 42/150\n",
      "26810/26810 [==============================] - 1s 26us/step - loss: 2.0643 - acc: 3.7300e-05\n",
      "Epoch 43/150\n",
      "26810/26810 [==============================] - 1s 28us/step - loss: 2.0642 - acc: 3.7300e-05\n",
      "Epoch 44/150\n",
      "26810/26810 [==============================] - 1s 28us/step - loss: 2.0648 - acc: 3.7300e-05\n",
      "Epoch 45/150\n",
      "26810/26810 [==============================] - 1s 28us/step - loss: 2.0639 - acc: 3.7300e-05\n",
      "Epoch 46/150\n",
      "26810/26810 [==============================] - 1s 27us/step - loss: 2.0621 - acc: 3.7300e-05\n",
      "Epoch 47/150\n",
      "26810/26810 [==============================] - 1s 26us/step - loss: 2.0628 - acc: 3.7300e-05\n",
      "Epoch 48/150\n",
      "26810/26810 [==============================] - 1s 27us/step - loss: 2.0605 - acc: 3.7300e-05\n",
      "Epoch 49/150\n",
      "26810/26810 [==============================] - 1s 27us/step - loss: 2.0596 - acc: 3.7300e-05\n",
      "Epoch 50/150\n",
      "26810/26810 [==============================] - 1s 28us/step - loss: 2.0597 - acc: 3.7300e-05\n",
      "Epoch 51/150\n",
      "26810/26810 [==============================] - 1s 27us/step - loss: 2.0588 - acc: 3.7300e-05\n",
      "Epoch 52/150\n",
      "26810/26810 [==============================] - 1s 30us/step - loss: 2.0581 - acc: 3.7300e-05\n",
      "Epoch 53/150\n",
      "26810/26810 [==============================] - 1s 26us/step - loss: 2.0580 - acc: 3.7300e-05\n",
      "Epoch 54/150\n",
      "26810/26810 [==============================] - 1s 27us/step - loss: 2.0564 - acc: 3.7300e-05\n",
      "Epoch 55/150\n",
      "26810/26810 [==============================] - 1s 29us/step - loss: 2.0557 - acc: 3.7300e-05\n",
      "Epoch 56/150\n",
      "26810/26810 [==============================] - 1s 27us/step - loss: 2.0541 - acc: 3.7300e-05\n",
      "Epoch 57/150\n",
      "26810/26810 [==============================] - 1s 26us/step - loss: 2.0538 - acc: 3.7300e-05\n",
      "Epoch 58/150\n",
      "26810/26810 [==============================] - 1s 25us/step - loss: 2.0528 - acc: 3.7300e-05\n",
      "Epoch 59/150\n",
      "26810/26810 [==============================] - 1s 28us/step - loss: 2.0521 - acc: 3.7300e-05\n",
      "Epoch 60/150\n",
      "26810/26810 [==============================] - 1s 28us/step - loss: 2.0505 - acc: 3.7300e-05\n",
      "Epoch 61/150\n",
      "26810/26810 [==============================] - 1s 28us/step - loss: 2.0507 - acc: 3.7300e-05\n",
      "Epoch 62/150\n",
      "26810/26810 [==============================] - 1s 28us/step - loss: 2.0511 - acc: 3.7300e-05\n",
      "Epoch 63/150\n",
      "26810/26810 [==============================] - 1s 28us/step - loss: 2.0493 - acc: 3.7300e-05\n",
      "Epoch 64/150\n",
      "26810/26810 [==============================] - 1s 28us/step - loss: 2.0492 - acc: 3.7300e-05\n",
      "Epoch 65/150\n",
      "26810/26810 [==============================] - 1s 28us/step - loss: 2.0469 - acc: 3.7300e-05\n",
      "Epoch 66/150\n",
      "26810/26810 [==============================] - 1s 28us/step - loss: 2.0469 - acc: 3.7300e-05\n",
      "Epoch 67/150\n",
      "26810/26810 [==============================] - 1s 27us/step - loss: 2.0458 - acc: 3.7300e-05\n",
      "Epoch 68/150\n",
      "26810/26810 [==============================] - 1s 26us/step - loss: 2.0440 - acc: 3.7300e-05\n",
      "Epoch 69/150\n",
      "26810/26810 [==============================] - 1s 27us/step - loss: 2.0439 - acc: 7.4599e-05\n",
      "Epoch 70/150\n",
      "26810/26810 [==============================] - 1s 28us/step - loss: 2.0420 - acc: 3.7300e-05\n",
      "Epoch 71/150\n",
      "26810/26810 [==============================] - 1s 28us/step - loss: 2.0413 - acc: 3.7300e-05\n",
      "Epoch 72/150\n",
      "26810/26810 [==============================] - 1s 27us/step - loss: 2.0418 - acc: 3.7300e-05\n",
      "Epoch 73/150\n",
      "26810/26810 [==============================] - 1s 28us/step - loss: 2.0404 - acc: 3.7300e-05\n",
      "Epoch 74/150\n",
      "26810/26810 [==============================] - 1s 27us/step - loss: 2.0393 - acc: 3.7300e-05\n",
      "Epoch 75/150\n",
      "26810/26810 [==============================] - 1s 28us/step - loss: 2.0397 - acc: 3.7300e-05\n",
      "Epoch 76/150\n",
      "26810/26810 [==============================] - 1s 28us/step - loss: 2.0390 - acc: 7.4599e-05\n",
      "Epoch 77/150\n",
      "26810/26810 [==============================] - 1s 26us/step - loss: 2.0381 - acc: 7.4599e-05\n",
      "Epoch 78/150\n",
      "26810/26810 [==============================] - 1s 25us/step - loss: 2.0371 - acc: 3.7300e-05\n",
      "Epoch 79/150\n",
      "26810/26810 [==============================] - 1s 26us/step - loss: 2.0362 - acc: 3.7300e-05\n",
      "Epoch 80/150\n",
      "26810/26810 [==============================] - 1s 27us/step - loss: 2.0354 - acc: 3.7300e-05\n",
      "Epoch 81/150\n",
      "26810/26810 [==============================] - 1s 28us/step - loss: 2.0348 - acc: 3.7300e-05\n",
      "Epoch 82/150\n",
      "26810/26810 [==============================] - 1s 25us/step - loss: 2.0347 - acc: 3.7300e-05\n",
      "Epoch 83/150\n",
      "26810/26810 [==============================] - 1s 26us/step - loss: 2.0348 - acc: 3.7300e-05\n",
      "Epoch 84/150\n",
      "26810/26810 [==============================] - 1s 27us/step - loss: 2.0326 - acc: 3.7300e-05\n",
      "Epoch 85/150\n",
      "26810/26810 [==============================] - 1s 27us/step - loss: 2.0305 - acc: 3.7300e-05\n",
      "Epoch 86/150\n",
      "26810/26810 [==============================] - 1s 29us/step - loss: 2.0338 - acc: 3.7300e-05\n",
      "Epoch 87/150\n",
      "26810/26810 [==============================] - 1s 25us/step - loss: 2.0310 - acc: 3.7300e-05\n",
      "Epoch 88/150\n",
      "26810/26810 [==============================] - 1s 27us/step - loss: 2.0311 - acc: 7.4599e-05\n",
      "Epoch 89/150\n",
      "26810/26810 [==============================] - 1s 26us/step - loss: 2.0295 - acc: 3.7300e-05\n",
      "Epoch 90/150\n",
      "26810/26810 [==============================] - 1s 26us/step - loss: 2.0275 - acc: 3.7300e-05\n",
      "Epoch 91/150\n",
      "26810/26810 [==============================] - 1s 26us/step - loss: 2.0288 - acc: 3.7300e-05\n",
      "Epoch 92/150\n",
      "26810/26810 [==============================] - 1s 26us/step - loss: 2.0288 - acc: 3.7300e-05\n",
      "Epoch 93/150\n",
      "26810/26810 [==============================] - 1s 27us/step - loss: 2.0283 - acc: 3.7300e-05\n",
      "Epoch 94/150\n",
      "26810/26810 [==============================] - 1s 26us/step - loss: 2.0266 - acc: 3.7300e-05\n",
      "Epoch 95/150\n",
      "26810/26810 [==============================] - 1s 27us/step - loss: 2.0251 - acc: 3.7300e-05\n",
      "Epoch 96/150\n",
      "26810/26810 [==============================] - 1s 27us/step - loss: 2.0241 - acc: 3.7300e-05\n",
      "Epoch 97/150\n",
      "26810/26810 [==============================] - 1s 27us/step - loss: 2.0260 - acc: 3.7300e-05\n",
      "Epoch 98/150\n",
      "26810/26810 [==============================] - 1s 28us/step - loss: 2.0256 - acc: 7.4599e-05\n",
      "Epoch 99/150\n",
      "26810/26810 [==============================] - 1s 28us/step - loss: 2.0241 - acc: 7.4599e-05\n",
      "Epoch 100/150\n",
      "26810/26810 [==============================] - 1s 29us/step - loss: 2.0218 - acc: 7.4599e-05\n",
      "Epoch 101/150\n",
      "26810/26810 [==============================] - 1s 26us/step - loss: 2.0225 - acc: 7.4599e-05\n",
      "Epoch 102/150\n",
      "26810/26810 [==============================] - 1s 27us/step - loss: 2.0206 - acc: 3.7300e-05\n",
      "Epoch 103/150\n",
      "26810/26810 [==============================] - 1s 26us/step - loss: 2.0211 - acc: 3.7300e-05\n",
      "Epoch 104/150\n",
      "26810/26810 [==============================] - 1s 25us/step - loss: 2.0195 - acc: 7.4599e-05\n",
      "Epoch 105/150\n",
      "26810/26810 [==============================] - 1s 25us/step - loss: 2.0199 - acc: 3.7300e-05\n",
      "Epoch 106/150\n",
      "26810/26810 [==============================] - 1s 26us/step - loss: 2.0191 - acc: 7.4599e-05\n",
      "Epoch 107/150\n",
      "26810/26810 [==============================] - 1s 26us/step - loss: 2.0192 - acc: 7.4599e-05\n",
      "Epoch 108/150\n",
      "26810/26810 [==============================] - 1s 26us/step - loss: 2.0182 - acc: 7.4599e-05\n",
      "Epoch 109/150\n",
      "26810/26810 [==============================] - 1s 25us/step - loss: 2.0178 - acc: 3.7300e-05\n",
      "Epoch 110/150\n",
      "26810/26810 [==============================] - 1s 26us/step - loss: 2.0172 - acc: 3.7300e-05\n",
      "Epoch 111/150\n",
      "26810/26810 [==============================] - 1s 25us/step - loss: 2.0150 - acc: 3.7300e-05\n",
      "Epoch 112/150\n",
      "26810/26810 [==============================] - 1s 25us/step - loss: 2.0157 - acc: 3.7300e-05\n",
      "Epoch 113/150\n",
      "26810/26810 [==============================] - 1s 25us/step - loss: 2.0150 - acc: 0.0000e+00\n",
      "Epoch 114/150\n",
      "26810/26810 [==============================] - 1s 26us/step - loss: 2.0144 - acc: 3.7300e-05\n",
      "Epoch 115/150\n",
      "26810/26810 [==============================] - 1s 25us/step - loss: 2.0135 - acc: 7.4599e-05\n",
      "Epoch 116/150\n",
      "26810/26810 [==============================] - 1s 25us/step - loss: 2.0131 - acc: 3.7300e-05\n",
      "Epoch 117/150\n",
      "26810/26810 [==============================] - 1s 25us/step - loss: 2.0132 - acc: 7.4599e-05\n",
      "Epoch 118/150\n",
      "26810/26810 [==============================] - 1s 25us/step - loss: 2.0113 - acc: 7.4599e-05\n",
      "Epoch 119/150\n",
      "26810/26810 [==============================] - 1s 25us/step - loss: 2.0123 - acc: 3.7300e-05\n",
      "Epoch 120/150\n",
      "26810/26810 [==============================] - 1s 25us/step - loss: 2.0117 - acc: 3.7300e-05\n",
      "Epoch 121/150\n",
      "26810/26810 [==============================] - 1s 26us/step - loss: 2.0109 - acc: 3.7300e-05\n",
      "Epoch 122/150\n",
      "26810/26810 [==============================] - 1s 25us/step - loss: 2.0099 - acc: 7.4599e-05\n",
      "Epoch 123/150\n",
      "26810/26810 [==============================] - 1s 25us/step - loss: 2.0079 - acc: 3.7300e-05\n",
      "Epoch 124/150\n",
      "26810/26810 [==============================] - 1s 25us/step - loss: 2.0085 - acc: 3.7300e-05\n",
      "Epoch 125/150\n",
      "26810/26810 [==============================] - 1s 25us/step - loss: 2.0075 - acc: 7.4599e-05\n",
      "Epoch 126/150\n",
      "26810/26810 [==============================] - 1s 25us/step - loss: 2.0083 - acc: 7.4599e-05\n",
      "Epoch 127/150\n",
      "26810/26810 [==============================] - 1s 25us/step - loss: 2.0060 - acc: 7.4599e-05\n",
      "Epoch 128/150\n",
      "26810/26810 [==============================] - 1s 26us/step - loss: 2.0069 - acc: 3.7300e-05\n",
      "Epoch 129/150\n",
      "26810/26810 [==============================] - 1s 26us/step - loss: 2.0057 - acc: 7.4599e-05\n",
      "Epoch 130/150\n",
      "26810/26810 [==============================] - 1s 24us/step - loss: 2.0053 - acc: 7.4599e-05\n",
      "Epoch 131/150\n",
      "26810/26810 [==============================] - 1s 24us/step - loss: 2.0039 - acc: 7.4599e-05\n",
      "Epoch 132/150\n",
      "26810/26810 [==============================] - 1s 28us/step - loss: 2.0035 - acc: 7.4599e-05\n",
      "Epoch 133/150\n",
      "26810/26810 [==============================] - 1s 35us/step - loss: 2.0040 - acc: 7.4599e-05\n",
      "Epoch 134/150\n",
      "26810/26810 [==============================] - 1s 32us/step - loss: 2.0024 - acc: 7.4599e-05\n",
      "Epoch 135/150\n",
      "26810/26810 [==============================] - 1s 33us/step - loss: 2.0029 - acc: 3.7300e-05\n",
      "Epoch 136/150\n",
      "26810/26810 [==============================] - 1s 34us/step - loss: 2.0010 - acc: 7.4599e-05\n",
      "Epoch 137/150\n",
      "26810/26810 [==============================] - 1s 34us/step - loss: 2.0020 - acc: 7.4599e-05\n",
      "Epoch 138/150\n",
      "26810/26810 [==============================] - 1s 34us/step - loss: 1.9996 - acc: 3.7300e-05\n",
      "Epoch 139/150\n",
      "26810/26810 [==============================] - 1s 30us/step - loss: 2.0003 - acc: 7.4599e-05\n",
      "Epoch 140/150\n",
      "26810/26810 [==============================] - 1s 28us/step - loss: 1.9983 - acc: 7.4599e-05\n",
      "Epoch 141/150\n",
      "26810/26810 [==============================] - 1s 29us/step - loss: 1.9976 - acc: 3.7300e-05\n",
      "Epoch 142/150\n",
      "26810/26810 [==============================] - 1s 29us/step - loss: 1.9981 - acc: 3.7300e-05\n",
      "Epoch 143/150\n",
      "26810/26810 [==============================] - 1s 28us/step - loss: 1.9988 - acc: 3.7300e-05\n",
      "Epoch 144/150\n",
      "26810/26810 [==============================] - 1s 32us/step - loss: 1.9965 - acc: 3.7300e-05\n",
      "Epoch 145/150\n",
      "26810/26810 [==============================] - 1s 29us/step - loss: 1.9958 - acc: 3.7300e-05\n",
      "Epoch 146/150\n",
      "26810/26810 [==============================] - 1s 33us/step - loss: 1.9967 - acc: 3.7300e-05\n",
      "Epoch 147/150\n",
      "26810/26810 [==============================] - 1s 29us/step - loss: 1.9944 - acc: 7.4599e-05\n",
      "Epoch 148/150\n",
      "26810/26810 [==============================] - 1s 32us/step - loss: 1.9939 - acc: 7.4599e-05\n",
      "Epoch 149/150\n",
      "26810/26810 [==============================] - 1s 30us/step - loss: 1.9962 - acc: 7.4599e-05\n",
      "Epoch 150/150\n",
      "26810/26810 [==============================] - 1s 28us/step - loss: 1.9927 - acc: 7.4599e-05\n",
      "1724/1724 [==============================] - 0s 66us/step\n"
     ]
    }
   ],
   "source": [
    "model2, error2, scaler2 = train_model(df_train, df_validation, neural_network(), 1, [False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4125969888165737"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = [model, error, scaler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1, error1, scaler1 = train_model(df_train, df_validation, linear_regression(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.309984279022966"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO REDUCE OVERFITTING: reduce degree of polynomial terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model might just not have the right things\n",
    "#to reorder columns:\n",
    "#cols = df.columns.tolist()\n",
    "#df = df[cols] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
